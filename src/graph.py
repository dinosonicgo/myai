# src/graph.py çš„ä¸­æ–‡è¨»é‡‹(v33.0 - æ°¸ä¹…æ€§è½Ÿç‚¸æ¶æ§‹)
# æ›´æ–°ç´€éŒ„:
# v33.0 (2025-10-03): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ“šã€Œæ°¸ä¹…æ€§è½Ÿç‚¸ã€èˆ‡ã€Œç›´æ¥RAGã€ç­–ç•¥ï¼Œå¾¹åº•é‡å¯«äº†ä¸»å°è©±åœ–ï¼Œç”¨ä¸€å€‹æ›´ç°¡å–®ã€æ›´ç·šæ€§çš„æ–°å·¥ä½œæµå–ä»£äº†èˆŠçš„ `preprocess_and_generate` é‚è¼¯ã€‚
# v32.0 (2025-10-15): [æ¶æ§‹ç°¡åŒ–] æ ¹æ“šã€Œæ°¸ä¹…æ€§è½Ÿç‚¸ã€ç­–ç•¥ï¼Œç§»é™¤äº† `classify_intent_node`ï¼Œä¸¦å°‡å…¶è·è²¬æ•´åˆé€²äº† `perceive_scene_node` å’Œ `validate_and_persist_node`ã€‚
# v31.0 (2025-10-15): [æ¶æ§‹é‡æ§‹] å°‡æ–°çš„ `classify_intent_node` ä½œç‚ºåœ–çš„å…¥å£é»ï¼Œå¯¦ç¾äº†ã€Œæ™ºèƒ½è½Ÿç‚¸ã€ç­–ç•¥ã€‚
import sys
import asyncio
import json
import re
from typing import Dict, List, Literal, Optional, Any

from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, END

from .ai_core import AILover
from .logger import logger
from .graph_state import ConversationGraphState, SetupGraphState
from . import lore_book, tools
from .schemas import (CharacterProfile, ExpansionDecision, 
                      UserInputAnalysis, SceneAnalysisResult, SceneCastingResult, 
                      WorldGenesisResult, IntentClassificationResult, StyleAnalysisResult,
                      ToolCall)
from .tool_context import tool_context
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- [v33.0 æ–°æ¶æ§‹] ä¸»å°è©±åœ– (Main Conversation Graph) çš„ç¯€é» ---

# å‡½å¼ï¼š[æ–°] å ´æ™¯æ„ŸçŸ¥èˆ‡ä¸Šä¸‹æ–‡æ¢å¾©ç¯€é» (v3.1 - é ç¨‹å„ªå…ˆç‹€æ…‹ä¿æŒ)
# æ›´æ–°ç´€éŒ„:
# v3.1 (2025-10-03): [ç½é›£æ€§BUGä¿®å¾©] æ ¹æ“šå ´æ™¯èª¤åˆ¤å•é¡Œï¼Œå¾¹åº•é‡æ§‹äº†æ­¤ç¯€é»çš„ç‹€æ…‹ç®¡ç†é‚è¼¯ã€‚æ–°ç‰ˆæœ¬å¼•å…¥äº†ã€Œé ç¨‹å„ªå…ˆã€çš„ç‹€æ…‹ä¿æŒç­–ç•¥ï¼šç•¶è¦–è§’å·²è™•æ–¼é ç¨‹æ¨¡å¼æ™‚ï¼Œé™¤éä½¿ç”¨è€…ç™¼å‡ºéå¸¸æ˜ç¢ºçš„è¿”å›æœ¬åœ°çš„æŒ‡ä»¤ï¼ˆå¦‚ç›´æ¥èˆ‡AIäº’å‹•æˆ–ç™¼å‡ºç§»å‹•å‘½ä»¤ï¼‰ï¼Œå¦å‰‡è¦–è§’å°‡è¢«å¼·åˆ¶ä¿æŒåœ¨é ç¨‹ã€‚åŒæ™‚ï¼Œåœ¨ç¯€é»çš„æœ«å°¾å¢åŠ äº†ç‹€æ…‹æŒä¹…åŒ–é‚è¼¯ï¼Œç¢ºä¿ä»»ä½•è¦–è§’çš„è®Šæ›´éƒ½æœƒè¢«ç«‹å³å¯«å…¥è³‡æ–™åº«ï¼Œè§£æ±ºäº†å› ç‹€æ…‹ä¸Ÿå¤±å°è‡´çš„å ´æ™¯éŒ¯äº‚å•é¡Œã€‚
# v3.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œæ°¸ä¹…æ€§è½Ÿç‚¸ã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚
# v2.0 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] å¼•å…¥äº†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¦–è§’ä¿æŒã€‘ç­–ç•¥ã€‚
async def perceive_scene_node(state: ConversationGraphState) -> Dict:
    """[1] (å…¥å£) åˆ†æç”¨æˆ·è¾“å…¥ï¼Œè™•ç†é€£çºŒæ€§æŒ‡ä»¤ï¼Œæ¢å¾©ä¸Šä¸‹æ–‡å¿«ç…§ï¼Œä¸¦ä¿æŒå ´æ™¯è¦–è§’çš„é€£è²«æ€§ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    # ç¢ºä¿ messages åˆ—è¡¨ä¸ç‚ºç©º
    if not state.get('messages'):
        logger.error(f"[{user_id}] (Graph|1) ç‹€æ…‹ä¸­ç¼ºå°‘ 'messages'ï¼Œç„¡æ³•æ„ŸçŸ¥å ´æ™¯ã€‚")
        # æä¾›ä¸€å€‹å®‰å…¨çš„é è¨­å€¼
        return {"scene_analysis": SceneAnalysisResult(viewing_mode='local', reasoning='é”™è¯¯ï¼šçŠ¶æ€ä¸­ç¼ºå°‘ messagesã€‚', action_summary="")}

    user_input = state['messages'][-1].content.strip() if state['messages'] else ""
    logger.info(f"[{user_id}] (Graph|1) Node: perceive_scene -> æ­£åœ¨æ„ŸçŸ¥åœºæ™¯ä¸æ¢å¤ä¸Šä¸‹æ–‡...")

    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|1) ai_core.profile æœªåŠ è½½ï¼Œæ— æ³•æ„ŸçŸ¥åœºæ™¯ã€‚")
        return {"scene_analysis": SceneAnalysisResult(viewing_mode='local', reasoning='é”™è¯¯ï¼šAI profile æœªåŠ è½½ã€‚', action_summary=user_input)}

    # [v3.1 æ ¸å¿ƒä¿®æ­£] ç„¡æ¢ä»¶å¾è³‡æ–™åº«æ¢å¾©çš„ GameState é–‹å§‹
    gs = ai_core.profile.game_state
    logger.info(f"[{user_id}] (Graph|1) å·²å¾æŒä¹…åŒ–å­˜å„²ä¸­æ¢å¾©ç•¶å‰ç‹€æ…‹ï¼šviewing_mode='{gs.viewing_mode}', remote_target='{gs.remote_target_path}'")
    
    # --- æ­¥é©Ÿ 1: è™•ç†é€£çºŒæ€§æŒ‡ä»¤èˆ‡ä¸Šä¸‹æ–‡æ¢å¾© ---
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.lower().startswith(kw) for kw in continuation_keywords):
        logger.info(f"[{user_id}] (Graph|1) æª¢æ¸¬åˆ°é€£çºŒæ€§æŒ‡ä»¤ï¼Œå°‡ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ä¸¦æ¢å¾©ä¸Šä¸‹æ–‡å¿«ç…§ã€‚")
        
        scene_analysis = SceneAnalysisResult(
            viewing_mode=gs.viewing_mode,
            reasoning="ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ã€‚",
            target_location_path=gs.remote_target_path,
            action_summary=user_input
        )
        
        if ai_core.last_context_snapshot:
            logger.info(f"[{user_id}] (Graph|1) [ä¸Šä¸‹æ–‡æ¢å¾©] æˆåŠŸæ¢å¾©ä¸Šä¸€è¼ªçš„ä¸Šä¸‹æ–‡å¿«ç…§ã€‚")
            return {
                "scene_analysis": scene_analysis,
                "raw_lore_objects": ai_core.last_context_snapshot.get("raw_lore_objects", []),
                "last_response_text": ai_core.last_context_snapshot.get("last_response_text", None)
            }
        else:
            logger.warning(f"[{user_id}] (Graph|1) [ä¸Šä¸‹æ–‡æ¢å¾©] æœªæ‰¾åˆ°ä¸Šä¸€è¼ªçš„ä¸Šä¸‹æ–‡å¿«ç…§ï¼Œå°‡é‡æ–°æŸ¥è©¢ LOREã€‚")
            return {"scene_analysis": scene_analysis}

    # --- æ­¥é©Ÿ 2: ä½¿ç”¨ LLM æ™ºèƒ½æ¨æ–·ä½¿ç”¨è€…æ˜¯å¦æ„åœ–è§€å¯Ÿé ç¨‹ ---
    new_viewing_mode = 'local'
    new_target_path = None
    final_reasoning = "å ´æ™¯æ„ŸçŸ¥å®Œæˆã€‚"

    try:
        location_chain_prompt = ai_core.get_location_extraction_prompt()
        full_prompt = ai_core._safe_format_prompt(location_chain_prompt, {"user_input": user_input})
        
        from .schemas import SceneLocationExtraction
        location_result = await ai_core.ainvoke_with_rotation(
            full_prompt, 
            output_schema=SceneLocationExtraction,
            retry_strategy='euphemize'
        )
        if location_result and location_result.has_explicit_location and location_result.location_path:
            final_reasoning = f"LLM æ„ŸçŸ¥æˆåŠŸã€‚æ¨æ–·å‡ºçš„ç›®æ¨™åœ°é»: {location_result.location_path}"
            logger.info(f"[{user_id}] (Graph|1) {final_reasoning}")
            new_target_path = location_result.location_path
            new_viewing_mode = 'remote'
    except Exception as e:
        final_reasoning = f"åœ°é»æ¨æ–·éˆå¤±æ•—: {e}ï¼Œå°‡å›é€€åˆ°åŸºæœ¬é‚è¼¯ã€‚"
        logger.warning(f"[{user_id}] (Graph|1) {final_reasoning}")

    # --- æ­¥é©Ÿ 3: [v3.1 æ ¸å¿ƒä¿®æ­£] æ‡‰ç”¨ã€Œé ç¨‹å„ªå…ˆã€çš„ç‹€æ…‹ä¿æŒé‚è¼¯ ---
    final_viewing_mode = gs.viewing_mode
    final_target_path = gs.remote_target_path

    if gs.viewing_mode == 'remote':
        # åªæœ‰åœ¨éå¸¸æ˜ç¢ºçš„æƒ…æ³ä¸‹æ‰åˆ‡æ›å› local
        is_explicit_local_move = any(user_input.startswith(kw) for kw in ["å»", "å‰å¾€", "ç§»å‹•åˆ°", "æ—…è¡Œåˆ°", "æˆ‘"])
        is_direct_ai_interaction = ai_core.profile.ai_profile.name in user_input
        
        if is_explicit_local_move or is_direct_ai_interaction:
            final_viewing_mode = 'local'
            final_target_path = None
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹åˆ‡æ›] æª¢æ¸¬åˆ°æ˜ç¢ºçš„æœ¬åœ°æŒ‡ä»¤ï¼Œå°æ¼”è¦–è§’å¾ 'remote' åˆ‡æ›å› 'local'ã€‚")
        elif new_viewing_mode == 'remote' and new_target_path and new_target_path != gs.remote_target_path:
            # å¦‚æœä»åœ¨é ç¨‹æ¨¡å¼ï¼Œä½†ç›®æ¨™è®Šäº†ï¼Œå‰‡æ›´æ–°ç›®æ¨™
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹æ›´æ–°] åœ¨é ç¨‹æ¨¡å¼ä¸‹ï¼Œæ›´æ–°äº†è§€å¯Ÿç›®æ¨™åœ°é»ç‚º: {final_target_path}")
        else:
            # å°æ–¼ "å¡è“®å‘¢?" é€™é¡æ¨¡ç³ŠæŒ‡ä»¤ï¼Œä¿æŒ remote æ¨¡å¼ä¸è®Š
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹ä¿æŒ] æœªæª¢æ¸¬åˆ°æ˜ç¢ºçš„æœ¬åœ°åˆ‡æ›ä¿¡è™Ÿï¼Œå°æ¼”è¦–è§’ä¿æŒç‚º 'remote'ã€‚")
    else: # gs.viewing_mode == 'local'
        # å¾ local åˆ‡æ›åˆ° remote çš„æ¢ä»¶ä¿æŒä¸è®Š
        if new_viewing_mode == 'remote' and new_target_path:
            final_viewing_mode = 'remote'
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹åˆ‡æ›] æª¢æ¸¬åˆ°é ç¨‹æè¿°æŒ‡ä»¤ï¼Œå°æ¼”è¦–è§’å¾ 'local' åˆ‡æ›åˆ° 'remote'ã€‚ç›®æ¨™: {final_target_path}")

    # --- æ­¥é©Ÿ 4: [v3.1 æ ¸å¿ƒä¿®æ­£] æŒä¹…åŒ–ç‹€æ…‹è®Šæ›´ ---
    if gs.viewing_mode != final_viewing_mode or gs.remote_target_path != final_target_path:
        logger.info(f"[{user_id}] (Graph|1) [æŒä¹…åŒ–] æª¢æ¸¬åˆ°ç‹€æ…‹è®Šæ›´ï¼Œæ­£åœ¨å°‡æ–°çš„ GameState å¯«å…¥è³‡æ–™åº«...")
        gs.viewing_mode = final_viewing_mode
        gs.remote_target_path = final_target_path
        await ai_core.update_and_persist_profile({'game_state': gs.model_dump()})
        logger.info(f"[{user_id}] (Graph|1) [æŒä¹…åŒ–] ç‹€æ…‹å·²æˆåŠŸä¿å­˜ã€‚")
    
    scene_analysis = SceneAnalysisResult(
        viewing_mode=gs.viewing_mode,
        reasoning=final_reasoning,
        target_location_path=gs.remote_target_path,
        action_summary=user_input
    )
    return {"scene_analysis": scene_analysis}
# å‡½å¼ï¼š[æ–°] å ´æ™¯æ„ŸçŸ¥èˆ‡ä¸Šä¸‹æ–‡æ¢å¾©ç¯€é» (v3.1 - é ç¨‹å„ªå…ˆç‹€æ…‹ä¿æŒ) çµæŸ





# å‡½å¼ï¼š[æ–°] è¨˜æ†¶èˆ‡ LORE æŸ¥è©¢ç¯€é» (v5.2 - è·è²¬é™ç´š)
# æ›´æ–°ç´€éŒ„:
# v5.2 (2025-10-03): [æ¶æ§‹ç°¡åŒ–] æ ¹æ“šã€Œä¸Šä¸‹æ–‡æ±¡æŸ“ã€åˆ†æï¼Œå¾¹åº•ç§»é™¤äº†æ­¤ç¯€é»æŸ¥è©¢çµæ§‹åŒ– LORE (`_query_lore_from_entities`) çš„è·è²¬ã€‚æ­¤ç¯€é»ç¾åœ¨çš„å”¯ä¸€ä»»å‹™æ˜¯åŸ·è¡Œ RAG æª¢ç´¢ï¼Œå°‡çµæ§‹åŒ– LORE çš„æ•¸æ“šèˆ‡ç”Ÿæˆ Prompt å¾¹åº•è§£è€¦ï¼Œä»¥å¯¦ç¾æ›´ç´”æ·¨ã€æ›´æ¥è¿‘ã€ŒRAG ç›´é€šã€çš„ä¸Šä¸‹æ–‡ç’°å¢ƒã€‚
# v5.1 (2025-10-03): [ç½é›£æ€§BUGä¿®å¾©] æ ¹æ“š AttributeErrorï¼Œä¿®æ­£äº†åœ¨æºé ­æ¸…æ´—æ­¥é©Ÿä¸­å° `ainvoke_with_rotation` çš„å‘¼å«æ–¹å¼ã€‚
# v5.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œç›´æ¥RAGã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚
async def retrieve_and_query_node(state: ConversationGraphState) -> Dict:
    """[2] (è·è²¬é™ç´š) åƒ…åŸ·è¡Œ RAG æª¢ç´¢ï¼Œä¸å†æŸ¥è©¢çµæ§‹åŒ– LOREã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    
    # æª¢æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²è¢«æ¢å¾© (æ­¤é‚è¼¯ä¿æŒä¸è®Š)
    if state.get('raw_lore_objects') is not None:
        logger.info(f"[{user_id}] (Graph|2) Node: retrieve_and_query -> æª¢æ¸¬åˆ°å·²æ¢å¾©çš„ LORE ä¸Šä¸‹æ–‡ï¼Œå°‡è·³éé‡æ–°æŸ¥è©¢ã€‚")
        rag_context_dict = await ai_core.retrieve_and_summarize_memories(user_input)
        return {
            "rag_context": rag_context_dict.get("summary", "ç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚"),
            # ä¿æŒå‚³é raw_lore_objects ä»¥ä¾›å¾ŒçºŒç¯€é»ï¼ˆå¦‚ expansionï¼‰ä½¿ç”¨
            "raw_lore_objects": state['raw_lore_objects'],
            "sanitized_query_for_tools": user_input,
            "last_response_text": state.get('last_response_text')
        }

    logger.info(f"[{user_id}] (Graph|2) Node: retrieve_and_query -> æ­£åœ¨åŸ·è¡Œ RAG æª¢ç´¢...")
    
    # æ¸…æ´—ä½¿ç”¨è€…è¼¸å…¥çš„é‚è¼¯ä¿æŒä¸è®Š
    sanitized_query = user_input
    try:
        literary_chain_prompt = ai_core.get_literary_euphemization_chain()
        full_prompt = ai_core._safe_format_prompt(literary_chain_prompt, {"dialogue_history": user_input})
        
        result = await ai_core.ainvoke_with_rotation(
            full_prompt, 
            retry_strategy='euphemize'
        )
        if result:
            sanitized_query = result
    except Exception as e:
        logger.warning(f"[{user_id}] (Graph|2) æºé ­æ¸…æ´—å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸå§‹è¼¸å…¥é€²è¡ŒæŸ¥è©¢ã€‚è©³ç´°éŒ¯èª¤: {type(e).__name__}")

    # åªåŸ·è¡Œ RAG æª¢ç´¢
    rag_context_dict = await ai_core.retrieve_and_summarize_memories(sanitized_query)
    rag_context_str = rag_context_dict.get("summary", "æ²’æœ‰æª¢ç´¢åˆ°ç›¸é—œçš„é•·æœŸè¨˜æ†¶ã€‚")

    # [v5.2 æ ¸å¿ƒä¿®æ­£] ç§»é™¤å° _query_lore_from_entities çš„èª¿ç”¨
    # æˆ‘å€‘ä»ç„¶éœ€è¦ä¸€å€‹ planning_subjects çš„ä¾†æºï¼Œé€™è£¡æˆ‘å€‘å¾ RAG çš„çµæœä¸­ç²—ç•¥æå–
    # æ³¨æ„ï¼šé€™ä¸€æ­¥é©Ÿæ˜¯ç‚ºäº†è®“ expansion_decision_and_execution_node èƒ½å¤ é‹ä½œ
    # ä½†é€™å€‹æ•¸æ“šä¸æœƒå†æ±¡æŸ“æœ€çµ‚çš„ç”Ÿæˆ Prompt
    all_lores = await lore_book.get_all_lores_for_user(user_id)
    
    logger.info(f"[{user_id}] (Graph|2) RAG æª¢ç´¢å®Œæˆã€‚")
    
    return {
        "rag_context": rag_context_str,
        # ç‚ºäº†è®“ expansion ç¯€é»èƒ½é‹ä½œï¼Œæˆ‘å€‘å‚³éä¸€å€‹ç©ºçš„ LORE åˆ—è¡¨
        # å¾ŒçºŒ expansion ç¯€é»æœƒè‡ªå·±è™•ç† LORE çš„ç²å–
        "raw_lore_objects": [], 
        "sanitized_query_for_tools": sanitized_query
    }
# å‡½å¼ï¼š[æ–°] è¨˜æ†¶èˆ‡ LORE æŸ¥è©¢ç¯€é» (v5.2 - è·è²¬é™ç´š) çµæŸ



# å‡½å¼ï¼š[æ–°] LORE æ“´å±•æ±ºç­–èˆ‡åŸ·è¡Œç¯€é» (v6.0 - è¼•é‡ç´šéª¨æ¶)
# æ›´æ–°ç´€éŒ„:
# v6.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œç›´æ¥RAGã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚å®ƒå–ä»£äº†èˆŠçš„é‡é‡ç´šã€Œå³æ™‚ç²¾ç…‰ã€ï¼Œæ”¹ç‚ºåŸ·è¡Œä¸€å€‹è¼•é‡ç´šçš„æ±ºç­–ï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦åœ¨å°è©±å‰ç‚ºä¸€å€‹å…¨æ–°çš„å¯¦é«”å‰µå»ºä¸€å€‹ã€Œéª¨æ¶ LOREã€ï¼Œä»¥é¿å… LLM æ†‘ç©ºæé€ ã€‚
# v5.0 (2025-10-10): [ç½é›£æ€§BUGä¿®å¾©] å¾¹åº•é‡æ§‹äº†ç¯€é»çš„è¼¸å‡ºé‚è¼¯ã€‚
async def expansion_decision_and_execution_node(state: ConversationGraphState) -> Dict:
    """[3] æ±ºç­–æ˜¯å¦éœ€è¦æ“´å±• LOREï¼Œå¦‚æœéœ€è¦ï¼Œå‰‡ç«‹å³åŸ·è¡Œè¼•é‡ç´šçš„éª¨æ¶å‰µå»ºã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    safe_query_text = state['sanitized_query_for_tools']
    raw_lore_objects = state.get('raw_lore_objects', [])
    logger.info(f"[{user_id}] (Graph|3) Node: expansion_decision_and_execution -> æ­£åœ¨æ±ºç­–æ˜¯å¦æ“´å±•LORE...")

    # ç²å–æ‰€æœ‰ç¾æœ‰ LORE çš„è¼•é‡ç´šæ‘˜è¦
    all_lores = await lore_book.get_all_lores_for_user(user_id)
    lightweight_lore_json = json.dumps(
        [{"name": lore.structured_content.get("name"), "description": (lore.narrative_content or "")[:50]} for lore in all_lores if lore.structured_content],
        ensure_ascii=False
    )
    
    decision_chain_prompt = ai_core.get_expansion_decision_chain()
    full_prompt = ai_core._safe_format_prompt(decision_chain_prompt, {
        "user_input": safe_query_text, 
        "existing_characters_json": lightweight_lore_json
    })
    
    decision = await ai_core.ainvoke_with_rotation(
        full_prompt, 
        output_schema=ExpansionDecision,
        retry_strategy='euphemize'
    )

    if not decision or not decision.should_expand:
        reason = decision.reasoning if decision else "æ±ºç­–éˆå¤±æ•—"
        logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæœï¼šç„¡éœ€æ“´å±•ã€‚ç†ç”±: {reason}")
        # å³ä½¿ä¸æ“´å±•ï¼Œä¹Ÿè¦å°‡æ‰€æœ‰ç¾æœ‰çš„ LORE å‚³éçµ¦ä¸‹ä¸€å€‹ç¯€é»
        return {"planning_subjects": [lore.structured_content for lore in all_lores if lore.structured_content]}

    logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæœï¼šéœ€è¦æ“´å±•ã€‚ç†ç”±: {decision.reasoning}ã€‚æ­£åœ¨åŸ·è¡ŒLOREæ“´å±•...")
    
    newly_created_lores = []
    try:
        # é€™è£¡æˆ‘å€‘èª¿ç”¨ä¸€å€‹æ›´é€šç”¨çš„ LORE æ“´å±•ç®¡ç·šï¼Œè€Œä¸åƒ…åƒ…æ˜¯å ´æ™¯é¸è§’
        expansion_prompt_template = ai_core.get_lore_expansion_pipeline_prompt()
        
        # æº–å‚™å·²çŸ¥ LORE å¯¦é«”åˆ—è¡¨ä¾›ç®¡ç·šåƒè€ƒ
        existing_lore_names = [lore.structured_content.get("name") for lore in all_lores if lore.structured_content and lore.structured_content.get("name")]
        
        expansion_prompt = ai_core._safe_format_prompt(expansion_prompt_template, {
            "user_input": safe_query_text,
            "existing_lore_json": json.dumps(existing_lore_names, ensure_ascii=False)
        })
        
        # è©²ç®¡ç·šæœƒè¿”å›ä¸€å€‹ CanonParsingResult ç‰©ä»¶
        expansion_result = await ai_core.ainvoke_with_rotation(
            expansion_prompt,
            output_schema=CanonParsingResult,
            retry_strategy='euphemize'
        )
        
        if expansion_result:
            # èª¿ç”¨ä¸€å€‹è¼”åŠ©å‡½å¼ä¾†è™•ç†ä¸¦ä¿å­˜æ‰€æœ‰æ–°å‰µå»ºçš„ LORE éª¨æ¶
            # æ³¨æ„ï¼šé€™è£¡çš„å¯¦ç¾å‡è¨­ _resolve_and_save_parsed_canon æ˜¯ä¸€å€‹èƒ½å¤ è™•ç† CanonParsingResult çš„æ–°è¼”åŠ©å‡½å¼
            # æˆ‘å€‘å°‡åœ¨å¾ŒçºŒæ­¥é©Ÿä¸­å®Œå–„å®ƒ
            newly_created_lores = await ai_core.parse_and_create_lore_from_canon_object(expansion_result)
            created_names = [lore.structured_content.get("name", "æœªçŸ¥") for lore in newly_created_lores if lore.structured_content]
            logger.info(f"[{user_id}] (Graph|3) æ“´å±•æˆåŠŸï¼Œå‰µå»ºäº† {len(created_names)} å€‹æ–°å¯¦é«”éª¨æ¶: {created_names}")
        
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|3) LOREæ“´å±•åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    
    # åˆä½µåŸå§‹ LORE å’Œæ–°å‰µå»ºçš„ LORE
    final_lore_objects = all_lores + newly_created_lores
    final_lores_map = {lore.key: lore for lore in final_lore_objects}
    
    # å°‡æ‰€æœ‰ LORE çš„çµæ§‹åŒ–éƒ¨åˆ†å‚³éçµ¦ä¸‹ä¸€å€‹ç¯€é»
    return {"planning_subjects": [lore.structured_content for lore in final_lores_map.values() if lore.structured_content]}
# å‡½å¼ï¼š[æ–°] LORE æ“´å±•æ±ºç­–èˆ‡åŸ·è¡Œç¯€é» (v6.0 - è¼•é‡ç´šéª¨æ¶) çµæŸ

# å‡½å¼ï¼š[æ–°] å‰ç½®å·¥å…·èª¿ç”¨ç¯€é» (v3.1 - åŸ·è¡Œé‚è¼¯ä¿®æ­£)
# æ›´æ–°ç´€éŒ„:
# v3.1 (2025-10-03): [ç½é›£æ€§BUGä¿®å¾©] æ ¹æ“šæ½›åœ¨çš„å¾ŒçºŒéŒ¯èª¤åˆ†æï¼Œå¾¹åº•é‡æ§‹äº†æ­¤å‡½å¼çš„å·¥å…·åŸ·è¡Œé‚è¼¯ã€‚æ–°ç‰ˆæœ¬ä¸å†éŒ¯èª¤åœ°å°‡ `ToolCallPlan` åŒ…è£æˆ `TurnPlan`ï¼Œè€Œæ˜¯ç›´æ¥èª¿ç”¨æ­£ç¢ºçš„ã€æ›´åº•å±¤çš„ `_execute_tool_call_plan` è¼”åŠ©å‡½å¼ä¾†åŸ·è¡Œå·¥å…·ï¼Œç¢ºä¿äº†å·¥å…·èª¿ç”¨æµç¨‹çš„æ­£ç¢ºæ€§å’Œé€£è²«æ€§ã€‚
# v3.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œç›´æ¥RAGã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚
# v2.1 (2025-10-14): [ç½é›£æ€§BUGä¿®å¾©] ä¿®æ­£äº† `CharacterAction` é©—è­‰éŒ¯èª¤ã€‚
async def preemptive_tool_call_node(state: ConversationGraphState) -> Dict:
    """[4] (å…¨æ–°) åˆ¤æ–·ä¸¦åŸ·è¡Œä½¿ç”¨è€…æŒ‡ä»¤ä¸­æ˜ç¢ºçš„ã€éœ€è¦æ”¹è®Šä¸–ç•Œç‹€æ…‹çš„å‹•ä½œã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    logger.info(f"[{user_id}] (Graph|4) Node: preemptive_tool_call -> æ­£åœ¨è§£æå‰ç½®å·¥å…·èª¿ç”¨...")

    tool_parsing_chain_prompt = ai_core.get_preemptive_tool_parsing_chain()
    # å¾ä¸Šä¸€å€‹ç¯€é»ç²å–æ‰€æœ‰ç›¸é—œè§’è‰²çš„åå­—
    character_list_str = ", ".join([ps.get("name", "") for ps in state.get("planning_subjects", []) if ps and ps.get("name")])
    
    full_prompt = ai_core._safe_format_prompt(tool_parsing_chain_prompt, {
        "user_input": user_input, 
        "character_list_str": character_list_str
    })
    
    from .schemas import ToolCallPlan
    tool_call_plan = await ai_core.ainvoke_with_rotation(
        full_prompt,
        output_schema=ToolCallPlan,
        retry_strategy='euphemize'
    )
    
    if not tool_call_plan or not tool_call_plan.plan:
        logger.info(f"[{user_id}] (Graph|4) æœªè§£æåˆ°æ˜ç¢ºçš„å·¥å…·èª¿ç”¨ã€‚")
        return {"tool_results": "ç³»çµ±äº‹ä»¶ï¼šç„¡å‰ç½®å·¥å…·è¢«èª¿ç”¨ã€‚"}

    logger.info(f"[{user_id}] (Graph|4) è§£æåˆ° {len(tool_call_plan.plan)} å€‹å·¥å…·èª¿ç”¨ï¼Œæº–å‚™åŸ·è¡Œ...")
    
    tool_context.set_context(user_id, ai_core)
    results_summary = ""
    try:
        # [v3.1 æ ¸å¿ƒä¿®æ­£] ç›´æ¥èª¿ç”¨æ­£ç¢ºçš„å·¥å…·è¨ˆç•«åŸ·è¡Œå‡½å¼
        if not ai_core.profile:
             raise Exception("AI Profileå°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•ç²å–ç•¶å‰åœ°é»ã€‚")
        current_location = ai_core.profile.game_state.location_path
        
        # æ³¨æ„ï¼š_execute_tool_call_plan ç¾åœ¨éœ€è¦èƒ½å¤ è™•ç†æ ¸å¿ƒå‹•ä½œå·¥å…·
        # æˆ‘å€‘å°‡åœ¨å¾ŒçºŒæ­¥é©Ÿä¸­ç¢ºä¿é€™ä¸€é»
        results_summary, _ = await ai_core._execute_tool_call_plan(tool_call_plan, current_location)
        
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        results_summary = f"ç³»çµ±äº‹ä»¶ï¼šå·¥å…·åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"
    finally:
        tool_context.set_context(None, None)
    
    logger.info(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œå®Œç•¢ã€‚")
    return {"tool_results": results_summary}
# å‡½å¼ï¼š[æ–°] å‰ç½®å·¥å…·èª¿ç”¨ç¯€é» (v3.1 - åŸ·è¡Œé‚è¼¯ä¿®æ­£) çµæŸ






# å‡½å¼ï¼š[æ–°] ä¸–ç•Œå¿«ç…§çµ„è£ç¯€é» (v3.2 - è·è²¬é™ç´š)
# æ›´æ–°ç´€éŒ„:
# v3.2 (2025-10-03): [æ¶æ§‹ç°¡åŒ–] æ ¹æ“šã€Œä¸Šä¸‹æ–‡æ±¡æŸ“ã€åˆ†æèˆ‡ã€ŒRAGç›´é€šã€ç­–ç•¥ï¼Œå¾¹åº•ç°¡åŒ–äº†æ­¤ç¯€é»çš„è·è²¬ã€‚å®ƒä¸å†å¾ `planning_subjects` æˆ– `tool_results` ä¸­æ‹¼æ¥è¤‡é›œçš„å ´æ™¯æè¿°ï¼Œè€Œæ˜¯åªè² è²¬å°‡æœ€æ ¸å¿ƒçš„è¨­å®šï¼ˆä¸–ç•Œè§€ã€AIè¨­å®šï¼‰å’Œæœ€é—œéµçš„ RAG æª¢ç´¢çµæœ (`retrieved_context`) å¡«å…¥æ¨¡æ¿ï¼Œå¾è€Œå‰µå»ºä¸€å€‹æ›´ç´”æ·¨ã€æ±¡æŸ“æ›´å°‘çš„ä¸Šä¸‹æ–‡å¿«ç…§ã€‚
# v3.1 (2025-10-03): [ç½é›£æ€§BUGä¿®å¾©] æ ¹æ“š KeyErrorï¼Œåœ¨ context_vars å­—å…¸ä¸­è£œå…¨äº† `explicit_character_files_context` éµã€‚
# v3.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œç›´æ¥RAGã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚
async def assemble_world_snapshot_node(state: ConversationGraphState) -> Dict:
    """[5] (è·è²¬é™ç´š) åƒ…çµ„è£ä¸€å€‹åŒ…å« RAG å’Œæ ¸å¿ƒè¨­å®šçš„ç´”æ·¨ä¸Šä¸‹æ–‡å¿«ç…§ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Graph|5) Node: assemble_world_snapshot -> æ­£åœ¨çµ„è£ã€ç´”æ·¨ç‰ˆã€‘ä¸Šä¸‹æ–‡å¿«ç…§...")
    
    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|5) è‡´å‘½éŒ¯èª¤: ai_core.profile æœªåŠ è¼‰ï¼")
        return {"world_snapshot": "éŒ¯èª¤ï¼šAI Profile ä¸Ÿå¤±ã€‚"}
        
    gs = ai_core.profile.game_state
    
    # [v3.2 æ ¸å¿ƒä¿®æ­£] context_vars ç¾åœ¨åªåŒ…å«æœ€æ ¸å¿ƒã€æœ€ä¸å¯èƒ½é€ æˆæ±¡æŸ“çš„ä¿¡æ¯
    context_vars = {
        'username': ai_core.profile.user_profile.name,
        'ai_name': ai_core.profile.ai_profile.name,
        'world_settings': ai_core.profile.world_settings or "æœªè¨­å®š",
        'ai_settings': ai_core.profile.ai_profile.description or "æœªè¨­å®š",
        'retrieved_context': state.get('rag_context', "ç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚"),
        
        # --- ä»¥ä¸‹ç‚ºæ¨¡æ¿ä¸­å¿…é ˆå­˜åœ¨ä½†åœ¨æ­¤ç°¡åŒ–æµç¨‹ä¸­å¯ä»¥æä¾›å®‰å…¨é è¨­å€¼çš„ä½”ä½ç¬¦ ---
        'possessions_context': f"åœ˜éšŠåº«å­˜: {', '.join(gs.inventory) or 'ç©ºçš„'}",
        'quests_context': "ç•¶å‰ç„¡ä»»å‹™ã€‚",
        'location_context': f"ç•¶å‰åœ°é»: {' > '.join(gs.location_path)}",
        'npc_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        'relevant_npc_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        'explicit_character_files_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        
        # --- å°æ¼”è¦–è§’ç›¸é—œä¿¡æ¯ä¿æŒä¸è®Š ---
        'player_location': " > ".join(gs.location_path),
        'viewing_mode': gs.viewing_mode,
        'remote_target_path_str': " > ".join(gs.remote_target_path) if gs.remote_target_path else "æœªæŒ‡å®š",
        'scene_rules_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰"
    }
    
    # ç¢ºä¿ world_snapshot_template å­˜åœ¨
    if not ai_core.world_snapshot_template:
        logger.error(f"[{user_id}] (Graph|5) è‡´å‘½éŒ¯èª¤: world_snapshot_template æœªåŠ è¼‰ï¼")
        return {"world_snapshot": "éŒ¯èª¤ï¼šä¸–ç•Œå¿«ç…§æ¨¡æ¿ä¸Ÿå¤±ã€‚"}

    final_world_snapshot = ai_core._safe_format_prompt(ai_core.world_snapshot_template, context_vars)
    
    logger.info(f"[{user_id}] (Graph|5) ã€ç´”æ·¨ç‰ˆã€‘ä¸Šä¸‹æ–‡å¿«ç…§çµ„è£å®Œç•¢ã€‚")
    return {"world_snapshot": final_world_snapshot}
# å‡½å¼ï¼š[æ–°] ä¸–ç•Œå¿«ç…§çµ„è£ç¯€é» (v3.2 - è·è²¬é™ç´š) çµæŸ




# å‡½å¼ï¼š[æ–°] æœ€çµ‚ç”Ÿæˆç¯€é» (v9.0 - é›™é‡å¼·åŒ–é¢¨æ ¼æ³¨å…¥)
# æ›´æ–°ç´€éŒ„:
# v9.0 (2025-10-12): [ç½é›£æ€§BUGä¿®å¾©] å¯¦ä½œäº†ã€Œé›™é‡å¼·åŒ–æ³¨å…¥ã€ç­–ç•¥ï¼Œå°‡ä½¿ç”¨è€…çš„è‡ªè¨‚é¢¨æ ¼æŒ‡ä»¤åŒæ™‚æ³¨å…¥åˆ°Promptçš„é ‚å±¤å’Œåº•å±¤ï¼Œä¸¦ä½¿ç”¨å¼·åˆ¶æ€§æªè¾­ï¼Œä»¥æœ€å¤§é™åº¦ç¢ºä¿LLMåœ¨å¸¸è¦å°è©±ä¸­ä¹Ÿèƒ½åš´æ ¼éµå®ˆé¢¨æ ¼è¦æ±‚ã€‚
# v8.1 (2025-10-03): [æ¶æ§‹ç°¡åŒ–] æ ¹æ“šã€ŒRAGç›´é€šã€ç­–ç•¥ï¼Œå¾¹åº•é‡å¯«äº†æ­¤ç¯€é»çš„ Prompt çµ„åˆé‚è¼¯ã€‚
# v8.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œæ°¸ä¹…æ€§è½Ÿç‚¸ã€æ¶æ§‹å‰µå»ºæ­¤ç¯€é»ã€‚
async def final_generation_node(state: ConversationGraphState) -> Dict:
    """[6] (é›™é‡å¼·åŒ–é¢¨æ ¼æ³¨å…¥) ç»„è£…ä¸€å€‹ç´”æ·¨çš„Promptï¼Œç¢ºä¿éµå¾ªé¢¨æ ¼æŒ‡ä»¤ï¼Œä¸¦èª¿ç”¨ç”Ÿæˆéˆã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    rag_context = state.get('rag_context', 'ï¼ˆç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚ï¼‰')
    user_input = state['messages'][-1].content
    logger.info(f"[{user_id}] (Graph|6) Node: final_generation -> å¯åŠ¨ã€é›™é‡å¼·åŒ–é¢¨æ ¼æ³¨å…¥ã€‘æœ€ç»ˆç”Ÿæˆæµç¨‹...")

    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|6) è‡´å‘½éŒ¯èª¤: ai_core.profile æœªåŠ è¼‰ï¼")
        return {"llm_response": "ï¼ˆéŒ¯èª¤ï¼šAI Profile ä¸Ÿå¤±ï¼Œç„¡æ³•ç”Ÿæˆå›æ‡‰ã€‚ï¼‰"}

    # --- æ­¥éª¤ 0: å‡†å¤‡é£æ ¼æŒ‡ä»¤å¼ºåŒ–å— ---
    style_prompt = ai_core.profile.response_style_prompt or "éå¸¸å…·é«”è©³ç´°æè¿°ï¼Œè±å¯Œå°è©±äº’å‹•"
    top_level_mandate = f"# === ã€ã€ã€âœï¸ çµ•å°é¢¨æ ¼å¼·åˆ¶ä»¤ã€‘ã€‘ã€‘ ===\n# ä½ çš„æ‰€æœ‰æ—ç™½å’Œå°è©±ï¼Œå…¶èªè¨€é¢¨æ ¼ã€è©³ç´°ç¨‹åº¦å’Œèªæ°£ï¼Œéƒ½ã€å¿…é ˆã€‘åš´æ ¼éµå¾ªä»¥ä¸‹æŒ‡ä»¤ï¼š\n# \"{style_prompt}\""
    recency_reinforcement = f"# === ã€ğŸ¬ æœ€çµ‚æŒ‡ä»¤ï¼šé¢¨æ ¼æé†’ã€‘ ===\n# è«‹åš´æ ¼éµå¾ªæ‚¨åœ¨Prompté ‚å±¤æ”¶åˆ°çš„ã€çµ•å°é¢¨æ ¼å¼·åˆ¶ä»¤ã€‘ï¼ˆâ€œ{style_prompt}â€ï¼‰ä¾†ç”Ÿæˆæ‚¨çš„å›æ‡‰ã€‚\n{ai_core.profile.ai_profile.name}:"

    historical_context = await _get_summarized_chat_history(ai_core, user_id)
    
    plot_anchor = "ï¼ˆç„¡ï¼‰"
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.strip().lower().startswith(kw) for kw in continuation_keywords):
        last_response = state.get('last_response_text')
        if last_response:
            plot_anchor = last_response
            logger.info(f"[{user_id}] (Graph|6) å·²æˆåŠŸç‚ºé€£çºŒæ€§æŒ‡ä»¤è¨­ç½®ã€åŠ‡æƒ…éŒ¨é»ã€‘ã€‚")

    final_prompt_template = """{core_protocol_prompt}
{top_level_style_mandate}

# === æƒ…å ±ç°¡å ± (ä¾†è‡ª RAG æª¢ç´¢) ===
{rag_context}
# === æƒ…å ±çµæŸ ===

# === åŠ‡æƒ…éŒ¨é» (ä¸Šä¸€å¹•çš„æœ€å¾Œå ´æ™¯) ===
{plot_anchor}
# === éŒ¨é»çµæŸ ===

# === æœ€è¿‘å°è©±æ­·å² ===
{historical_context}
# === æ­·å²çµæŸ ===

# === æœ¬å›åˆäº’å‹• ===
{username}: {latest_user_input}
{recency_style_reinforcement}"""

    full_prompt = ai_core._safe_format_prompt(
        final_prompt_template,
        {
            "core_protocol_prompt": ai_core.core_protocol_prompt,
            "top_level_style_mandate": top_level_mandate,
            "rag_context": rag_context,
            "plot_anchor": plot_anchor,
            "historical_context": historical_context,
            "username": ai_core.profile.user_profile.name,
            "latest_user_input": user_input,
            "recency_style_reinforcement": recency_reinforcement
        }
    )
    
    logger.info(f"[{user_id}] (Graph|6) [æ°¸ä¹…æ€§è½Ÿç‚¸] å·²å•Ÿç”¨ 'force' å‚™æ´ç­–ç•¥ä»¥æœ€å¤§åŒ–æˆåŠŸç‡ã€‚")
    
    final_response = await ai_core.ainvoke_with_rotation(
        full_prompt,
        retry_strategy='force',
        use_degradation=True
    )

    if not final_response or not final_response.strip():
        logger.critical(f"[{user_id}] (Graph|6) æ ¸å¿ƒç”Ÿæˆé“¾åœ¨æ‰€æœ‰ç­–ç•¥ä¹‹å¾Œæœ€çµ‚å¤±æ•—ï¼")
        final_response = "ï¼ˆæŠ±æ­‰ï¼Œæˆ‘å¥½åƒçªç„¶æ–­çº¿äº†ï¼Œè„‘æµ·ä¸­ä¸€ç‰‡ç©ºç™½...ï¼‰"
        
    logger.info(f"[{user_id}] (Graph|6) æœ€ç»ˆç”Ÿæˆæµç¨‹å®Œæˆã€‚")
    return {"llm_response": final_response}
# å‡½å¼ï¼š[æ–°] æœ€çµ‚ç”Ÿæˆç¯€é» çµæŸ




  

# å‡½å¼ï¼š[æ–°] é©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–ç¯€é» (v3.0 - ç”Ÿæˆèˆ‡å­¸ç¿’åˆ†é›¢)
# æ›´æ–°ç´€éŒ„:
# v3.0 (2025-10-03): [å…¨æ–°å‰µå»º] æ ¹æ“šã€Œç”Ÿæˆèˆ‡å­¸ç¿’åˆ†é›¢ã€åŸå‰‡å‰µå»ºæ­¤ç¯€é»ã€‚å®ƒä½œç‚ºåœ–çš„çµ‚é»ï¼Œè² è²¬æ‰€æœ‰äº‹å¾Œè™•ç†å·¥ä½œï¼šæ¸…ç†æ–‡æœ¬ã€ç•°æ­¥è§¸ç™¼èƒŒæ™¯ LORE æå–ã€ä¿å­˜å°è©±æ­·å²ï¼Œä¸¦ç‚ºä¸‹ä¸€è¼ªçš„é€£çºŒæ€§æŒ‡ä»¤å‰µå»ºé—œéµçš„ä¸Šä¸‹æ–‡å¿«ç…§ã€‚
# v2.8 (2025-10-15): [å¥å£¯æ€§] åœ¨æ­¤ç¯€é»çš„æœ«å°¾ï¼Œå‰µå»ºä¸¦å„²å­˜ä¸Šä¸‹æ–‡å¿«ç…§ã€‚
async def validate_and_persist_node(state: ConversationGraphState) -> Dict:
    """[7] æ¸…ç†æ–‡æœ¬ã€äº‹å¾Œ LORE æå–ã€ä¿å­˜å°è©±æ­·å²ï¼Œä¸¦ç‚ºä¸‹ä¸€è¼ªå‰µå»ºä¸Šä¸‹æ–‡å¿«ç…§ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    llm_response = state['llm_response']
    logger.info(f"[{user_id}] (Graph|7) Node: validate_and_persist -> æ­£åœ¨é©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–...")

    clean_response = llm_response.strip()
    
    # å‰µå»ºä¸Šä¸‹æ–‡å¿«ç…§ä»¥ä¾›èƒŒæ™¯ä»»å‹™ä½¿ç”¨
    snapshot_for_analysis = {
        "user_input": user_input,
        "final_response": clean_response,
    }
    
    # ç•°æ­¥è§¸ç™¼èƒŒæ™¯å­¸ç¿’ï¼ˆèª¿ç”¨æˆ‘å€‘åœ¨ç¬¬äº”éšæ®µé‡æ§‹çš„ã€çµ•å°å®‰å…¨çš„LOREæå–å‡½å¼ï¼‰
    asyncio.create_task(ai_core._background_lore_extraction(snapshot_for_analysis))

    # ä½¿ç”¨æˆ‘å€‘æ–°çš„æŒä¹…åŒ–å‡½å¼ä¿å­˜å°è©±æ­·å²
    # æ³¨æ„ï¼š_save_interaction_to_dbs å…§éƒ¨æœƒè™•ç†ç·¨ç¢¼å’Œé›™é‡æŒä¹…åŒ–
    await ai_core._save_interaction_to_dbs(f"ä½¿ç”¨è€…: {user_input}\n\nAI:\n{clean_response}")
    logger.info(f"[{user_id}] (Graph|7) å°è©±æ­·å²å·²æ›´æ–°ä¸¦é€²è¡Œé›™é‡æŒä¹…åŒ–ã€‚")

    # ç‚ºä¸‹ä¸€è¼ªçš„ "continue" æŒ‡ä»¤å‰µå»ºå¿«ç…§
    snapshot_for_next_turn = {
        "raw_lore_objects": state.get("raw_lore_objects", []),
        "last_response_text": clean_response
    }
    ai_core.last_context_snapshot = snapshot_for_next_turn
    logger.info(f"[{user_id}] (Graph|7) å·²ç‚ºä¸‹ä¸€è¼ªå‰µå»ºä¸Šä¸‹æ–‡å¿«ç…§ã€‚")

    logger.info(f"[{user_id}] (Graph|7) ç‹€æ…‹æŒä¹…åŒ–å®Œæˆã€‚")
    
    return {"final_output": clean_response}
# å‡½å¼ï¼š[æ–°] é©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–ç¯€é» çµæŸ





# å‡½å¼ï¼šç²å–æ‘˜è¦å¾Œçš„å°è©±æ­·å²
# æ›´æ–°ç´€éŒ„:
# v28.0 (2025-09-25): [ç½é›£æ€§BUGä¿®å¾©] æ¡ç”¨äº†å…¨æ–°çš„ã€æ›´å¼·å¤§çš„æ–‡å­¸è©•è«–å®¶éˆã€‚
async def _get_summarized_chat_history(ai_core: AILover, user_id: str, num_messages: int = 8) -> str:
    """æå–ä¸¦æ‘˜è¦æœ€è¿‘çš„å°è©±æ­·å²ï¼Œä¸¦å…§å»ºä¸€å€‹å¼·å¤§çš„ã€åŸºæ–¼ã€Œæ–‡å­¸è©•è«–å®¶ã€é‡å¯«çš„ NSFW å…§å®¹å®‰å…¨å‚™æ´æ©Ÿåˆ¶ã€‚"""
    if not ai_core.profile: return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
    
    scene_key = ai_core._get_scene_key()
    chat_history_manager = ai_core.scene_histories.get(scene_key, ChatMessageHistory())

    if not chat_history_manager.messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
        
    recent_messages = chat_history_manager.messages[-num_messages:]
    if not recent_messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"

    raw_history_text = "\n".join([f"{'ä½¿ç”¨è€…' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in recent_messages])

    try:
        literary_chain_prompt = ai_core.get_literary_euphemization_chain()
        summary = await ai_core.ainvoke_with_rotation(literary_chain_prompt, {"dialogue_history": raw_history_text}, retry_strategy='euphemize')

        if not summary or not summary.strip():
            raise Exception("Summarization returned empty content.")
            
        return f"ã€æœ€è¿‘å°è©±æ‘˜è¦ã€‘:\n{summary}"

    except Exception as e:
        logger.error(f"[{user_id}] (History Summarizer) ç”Ÿæˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚è¿”å›ä¸­æ€§æç¤ºã€‚")
        return "ï¼ˆæ­·å²å°è©±æ‘˜è¦å› é”™è¯¯è€Œç”Ÿæˆå¤±è´¥ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡å¯èƒ½ç¼ºå¤±ã€‚ï¼‰"
# å‡½å¼ï¼šç²å–æ‘˜è¦å¾Œçš„å°è©±æ­·å²

# å‡½å¼ï¼š[æ–°] åœ–çš„æ§‹å»º (v33.0 - ç·šæ€§å·¥ä½œæµ)
# æ›´æ–°ç´€éŒ„:
# v33.0 (2025-10-03): [é‡å¤§æ¶æ§‹é‡æ§‹] å‰µå»ºæ­¤å‡½å¼ï¼Œå°‡æ‰€æœ‰æ–°å‰µå»ºçš„ç¯€é»é€£æ¥æˆä¸€å€‹ç·šæ€§çš„ã€å–ä»£èˆŠ `preprocess_and_generate` æµç¨‹çš„ä¸»å°è©±åœ–ã€‚
def create_main_response_graph() -> StateGraph:
    """åˆ›å»ºå¹¶è¿æ¥æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ„å»ºæœ€ç»ˆçš„å¯¹è¯å›¾ã€‚"""
    graph = StateGraph(ConversationGraphState)
    
    graph.add_node("perceive_scene", perceive_scene_node)
    graph.add_node("retrieve_and_query", retrieve_and_query_node)
    graph.add_node("expansion_decision_and_execution", expansion_decision_and_execution_node)
    graph.add_node("preemptive_tool_call", preemptive_tool_call_node)
    graph.add_node("assemble_world_snapshot", assemble_world_snapshot_node)
    graph.add_node("final_generation", final_generation_node)
    graph.add_node("validate_and_persist", validate_and_persist_node)
    
    graph.set_entry_point("perceive_scene")
    
    graph.add_edge("perceive_scene", "retrieve_and_query")
    graph.add_edge("retrieve_and_query", "expansion_decision_and_execution")
    graph.add_edge("expansion_decision_and_execution", "preemptive_tool_call")
    graph.add_edge("preemptive_tool_call", "assemble_world_snapshot")
    graph.add_edge("assemble_world_snapshot", "final_generation")
    graph.add_edge("final_generation", "validate_and_persist")
    graph.add_edge("validate_and_persist", END)
    
    return graph.compile()
# å‡½å¼ï¼š[æ–°] åœ–çš„æ§‹å»º (v33.0 - ç·šæ€§å·¥ä½œæµ) çµæŸ

# --- Setup Graph (ä¿æŒä¸è®Š) ---
# å‡½å¼ï¼šè™•ç†ä¸–ç•Œè–ç¶“ç¯€é»
async def process_canon_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state['canon_text']
    logger.info(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> ç¯€é»å·²å•Ÿå‹•ã€‚")
    try:
        if canon_text:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æª¢æ¸¬åˆ°ä¸–ç•Œè–ç¶“æ–‡æœ¬ (é•·åº¦: {len(canon_text)})ï¼Œé–‹å§‹è™•ç†...")
            # æ³¨æ„ï¼šæ­¤è™• RAG ç´¢å¼•çš„æ§‹å»ºå·²è¢«ç§»è‡³æ›´é«˜å±¤çš„å”èª¿å™¨
            await ai_core.parse_and_create_lore_from_canon(canon_text)
            logger.info(f"[{user_id}] (Setup Graph|1/4) LORE æ™ºèƒ½è§£æå®Œæˆã€‚")
        else:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æœªæä¾›ä¸–ç•Œè–ç¶“æ–‡æœ¬ï¼Œè·³éè™•ç†ã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {}
# å‡½å¼ï¼šè™•ç†ä¸–ç•Œè–ç¶“ç¯€é» çµæŸ

# å‡½å¼ï¼šè£œå®Œè§’è‰²æª”æ¡ˆç¯€é»
async def complete_profiles_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»å·²å•Ÿå‹•ï¼Œæº–å‚™è£œå®Œè§’è‰²æª”æ¡ˆ...")
    try:
        await ai_core.complete_character_profiles()
        logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {}
# å‡½å¼ï¼šè£œå®Œè§’è‰²æª”æ¡ˆç¯€é» çµæŸ

# å‡½å¼ï¼šä¸–ç•Œå‰µä¸–ç¯€é»
async def world_genesis_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state.get('canon_text')
    logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»å·²å•Ÿå‹•...")
    genesis_result = None
    try:
        await ai_core.generate_world_genesis(canon_text=canon_text)
        logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {"genesis_result": genesis_result}
# å‡½å¼ï¼šä¸–ç•Œå‰µä¸–ç¯€é» çµæŸ

# å‡½å¼ï¼šç”Ÿæˆé–‹å ´ç™½ç¯€é»
async def generate_opening_scene_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state.get('canon_text')
    opening_scene = ""
    logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»å·²å•Ÿå‹•...")
    try:
        opening_scene = await ai_core.generate_opening_scene(canon_text=canon_text)
        if not opening_scene or not opening_scene.strip():
            opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
        logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
    return {"opening_scene": opening_scene}
# å‡½å¼ï¼šç”Ÿæˆé–‹å ´ç™½ç¯€é» çµæŸ




# å‡½å¼ï¼šå‰µå»ºè¨­å®šåœ– (v36.0 - çµ‚æ¥µç°¡åŒ–)
# æ›´æ–°ç´€éŒ„:
# v36.0 (2025-10-03): [é‡å¤§æ¶æ§‹ç°¡åŒ–] æ ¹æ“šã€Œæ±ºç­–èˆ‡åŸ·è¡Œåˆä¸€ã€çš„æœ€çµ‚ç­–ç•¥ï¼Œå¾å‰µä¸–åœ–ä¸­å¾¹åº•ç§»é™¤äº† `world_genesis` ç¯€é»ã€‚åˆå§‹åœ°é»çš„é¸æ“‡å’Œ LORE å‰µå»ºè·è²¬ï¼Œç¾å·²å®Œå…¨æ•´åˆé€² `generate_opening_scene` ç¯€é»å…§éƒ¨ã€‚æ­¤ä¿®æ”¹å°‡å‰µä¸–æµç¨‹ç°¡åŒ–ç‚ºå…©å€‹æ ¸å¿ƒæ­¥é©Ÿï¼ˆè£œå®Œæª”æ¡ˆ -> ç”Ÿæˆå ´æ™¯ï¼‰ï¼Œå¾æ ¹æœ¬ä¸Šè§£æ±ºäº†å› å¤šç¯€é»æ•¸æ“šæµä¸åŒæ­¥è€Œå°è‡´çš„åœ°é»ä¸ä¸€è‡´å•é¡Œï¼ŒåŒæ™‚æé«˜äº†å‰µä¸–æ•ˆç‡ã€‚
# v35.0 (2025-10-03): [é‡å¤§æ¶æ§‹é‡æ§‹] å¾¹åº•ç§»é™¤äº† `process_canon` ç¯€é»ï¼Œå¯¦ç¾äº† LORE çš„ã€Œå³æ™‚èˆ‡å¢é‡ã€å­¸ç¿’ã€‚
# v34.0 (2025-10-03): [é‡å¤§æ¶æ§‹ç°¡åŒ–] å¾å‰µä¸–åœ–ä¸­å¾¹åº•ç§»é™¤äº† `process_canon` ç¯€é»åŠå…¶ç›¸é—œçš„é‚Šã€‚
def create_setup_graph() -> StateGraph:
    """å‰µå»ºè¨­å®šåœ– (v36.0 - çµ‚æ¥µç°¡åŒ–ç‰ˆ)"""
    graph = StateGraph(SetupGraphState)

    graph.add_node("complete_profiles", complete_profiles_node)
    # [v36.0 æ ¸å¿ƒä¿®æ­£] å¾¹åº•ç§»é™¤ world_genesis ç¯€é»
    # graph.add_node("world_genesis", world_genesis_node)
    graph.add_node("generate_opening_scene", generate_opening_scene_node)
    
    graph.set_entry_point("complete_profiles")
    
    # [v36.0 æ ¸å¿ƒä¿®æ­£] èª¿æ•´é‚Šçš„é€£æ¥ï¼Œè·³é world_genesis
    graph.add_edge("complete_profiles", "generate_opening_scene")
    graph.add_edge("generate_opening_scene", END)
    
    return graph.compile()
# å‡½å¼ï¼šå‰µå»ºè¨­å®šåœ– çµæŸ






