# src/graph.py çš„ä¸­æ–‡è¨»é‡‹(v34.0 - è¿è§„å®¡æŸ¥å¾ªçŽ¯)
# æ›´æ–°ç´€éŒ„:
# v34.0 (2025-10-13): [ç¾éš¾æ€§BUGä¿®å¤] å¼•å…¥äº†`violation_check_node`å’Œ`should_regenerate`æ¡ä»¶åˆ†æ”¯ã€‚åœ¨AIç”Ÿæˆå›žåº”åŽï¼Œä¼šé¢å¤–è¿›è¡Œä¸€æ¬¡â€œä½¿ç”¨è€…ä¸»æƒâ€è¿è§„å®¡æŸ¥ï¼Œå¦‚æžœå‘çŽ°AIæ‰®æ¼”äº†ä½¿ç”¨è€…ï¼Œåˆ™ä¼šå¼ºåˆ¶é©³å›žå¹¶é‡æ–°ç”Ÿæˆï¼Œä»Žæ ¹æœ¬ä¸Šè§£å†³è§’è‰²æ‰®æ¼”æ··ä¹±çš„é—®é¢˜ã€‚
# v33.0 (2025-10-03): [é‡å¤§æž¶æ§‹é‡æ§‹] æ ¹æ“šã€Œæ°¸ä¹…æ€§è½Ÿç‚¸ã€èˆ‡ã€Œç›´æŽ¥RAGã€ç­–ç•¥ï¼Œå¾¹åº•é‡å¯«äº†ä¸»å°è©±åœ–ã€‚
# v32.0 (2025-10-15): [æž¶æ§‹ç°¡åŒ–] ç§»é™¤äº† `classify_intent_node`ã€‚

import sys
import asyncio
import json
import re
from typing import Dict, List, Literal, Optional, Any

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, END

from .ai_core import AILover
from .logger import logger
from .graph_state import ConversationGraphState, SetupGraphState
from . import lore_book, tools
from .schemas import (CharacterProfile, ExpansionDecision, 
                      UserInputAnalysis, SceneAnalysisResult, SceneCastingResult, 
                      WorldGenesisResult, IntentClassificationResult, StyleAnalysisResult,
                      ToolCall, ValidationResult)
from .tool_context import tool_context
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- ä¸»å°è©±åœ– (Main Conversation Graph) çš„ç¯€é»ž ---

async def perceive_scene_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    if not state.get('messages'):
        logger.error(f"[{user_id}] (Graph|1) ç‹€æ…‹ä¸­ç¼ºå°‘ 'messages'ï¼Œç„¡æ³•æ„ŸçŸ¥å ´æ™¯ã€‚")
        return {"scene_analysis": SceneAnalysisResult(viewing_mode='local', reasoning='é”™è¯¯ï¼šçŠ¶æ€ä¸­ç¼ºå°‘ messagesã€‚', action_summary="")}
    user_input = state['messages'][-1].content.strip() if state['messages'] else ""
    logger.info(f"[{user_id}] (Graph|1) Node: perceive_scene -> æ­£åœ¨æ„ŸçŸ¥åœºæ™¯ä¸Žæ¢å¤ä¸Šä¸‹æ–‡...")
    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|1) ai_core.profile æœªåŠ è½½ï¼Œæ— æ³•æ„ŸçŸ¥åœºæ™¯ã€‚")
        return {"scene_analysis": SceneAnalysisResult(viewing_mode='local', reasoning='é”™è¯¯ï¼šAI profile æœªåŠ è½½ã€‚', action_summary=user_input)}
    gs = ai_core.profile.game_state
    logger.info(f"[{user_id}] (Graph|1) å·²å¾žæŒä¹…åŒ–å­˜å„²ä¸­æ¢å¾©ç•¶å‰ç‹€æ…‹ï¼šviewing_mode='{gs.viewing_mode}', remote_target='{gs.remote_target_path}'")
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æŽ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.lower().startswith(kw) for kw in continuation_keywords):
        logger.info(f"[{user_id}] (Graph|1) æª¢æ¸¬åˆ°é€£çºŒæ€§æŒ‡ä»¤ï¼Œå°‡ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ä¸¦æ¢å¾©ä¸Šä¸‹æ–‡å¿«ç…§ã€‚")
        scene_analysis = SceneAnalysisResult(
            viewing_mode=gs.viewing_mode,
            reasoning="ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ã€‚",
            target_location_path=gs.remote_target_path,
            action_summary=user_input
        )
        if ai_core.last_context_snapshot:
            logger.info(f"[{user_id}] (Graph|1) [ä¸Šä¸‹æ–‡æ¢å¾©] æˆåŠŸæ¢å¾©ä¸Šä¸€è¼ªçš„ä¸Šä¸‹æ–‡å¿«ç…§ã€‚")
            return {
                "scene_analysis": scene_analysis,
                "raw_lore_objects": ai_core.last_context_snapshot.get("raw_lore_objects", []),
                "last_response_text": ai_core.last_context_snapshot.get("last_response_text", None)
            }
        else:
            logger.warning(f"[{user_id}] (Graph|1) [ä¸Šä¸‹æ–‡æ¢å¾©] æœªæ‰¾åˆ°ä¸Šä¸€è¼ªçš„ä¸Šä¸‹æ–‡å¿«ç…§ï¼Œå°‡é‡æ–°æŸ¥è©¢ LOREã€‚")
            return {"scene_analysis": scene_analysis}
    new_viewing_mode = 'local'
    new_target_path = None
    final_reasoning = "å ´æ™¯æ„ŸçŸ¥å®Œæˆã€‚"
    try:
        location_chain_prompt = ai_core.get_location_extraction_prompt()
        full_prompt = ai_core._safe_format_prompt(location_chain_prompt, {"user_input": user_input})
        from .schemas import SceneLocationExtraction
        location_result = await ai_core.ainvoke_with_rotation(
            full_prompt, 
            output_schema=SceneLocationExtraction,
            retry_strategy='euphemize'
        )
        if location_result and location_result.has_explicit_location and location_result.location_path:
            final_reasoning = f"LLM æ„ŸçŸ¥æˆåŠŸã€‚æŽ¨æ–·å‡ºçš„ç›®æ¨™åœ°é»ž: {location_result.location_path}"
            logger.info(f"[{user_id}] (Graph|1) {final_reasoning}")
            new_target_path = location_result.location_path
            new_viewing_mode = 'remote'
    except Exception as e:
        final_reasoning = f"åœ°é»žæŽ¨æ–·éˆå¤±æ•—: {e}ï¼Œå°‡å›žé€€åˆ°åŸºæœ¬é‚è¼¯ã€‚"
        logger.warning(f"[{user_id}] (Graph|1) {final_reasoning}")
    final_viewing_mode = gs.viewing_mode
    final_target_path = gs.remote_target_path
    if gs.viewing_mode == 'remote':
        is_explicit_local_move = any(user_input.startswith(kw) for kw in ["åŽ»", "å‰å¾€", "ç§»å‹•åˆ°", "æ—…è¡Œåˆ°", "æˆ‘"])
        is_direct_ai_interaction = ai_core.profile.ai_profile.name in user_input
        if is_explicit_local_move or is_direct_ai_interaction:
            final_viewing_mode = 'local'
            final_target_path = None
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹åˆ‡æ›] æª¢æ¸¬åˆ°æ˜Žç¢ºçš„æœ¬åœ°æŒ‡ä»¤ï¼Œå°Žæ¼”è¦–è§’å¾ž 'remote' åˆ‡æ›å›ž 'local'ã€‚")
        elif new_viewing_mode == 'remote' and new_target_path and new_target_path != gs.remote_target_path:
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹æ›´æ–°] åœ¨é ç¨‹æ¨¡å¼ä¸‹ï¼Œæ›´æ–°äº†è§€å¯Ÿç›®æ¨™åœ°é»žç‚º: {final_target_path}")
        else:
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹ä¿æŒ] æœªæª¢æ¸¬åˆ°æ˜Žç¢ºçš„æœ¬åœ°åˆ‡æ›ä¿¡è™Ÿï¼Œå°Žæ¼”è¦–è§’ä¿æŒç‚º 'remote'ã€‚")
    else: 
        if new_viewing_mode == 'remote' and new_target_path:
            final_viewing_mode = 'remote'
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) [ç‹€æ…‹åˆ‡æ›] æª¢æ¸¬åˆ°é ç¨‹æè¿°æŒ‡ä»¤ï¼Œå°Žæ¼”è¦–è§’å¾ž 'local' åˆ‡æ›åˆ° 'remote'ã€‚ç›®æ¨™: {final_target_path}")
    if gs.viewing_mode != final_viewing_mode or gs.remote_target_path != final_target_path:
        logger.info(f"[{user_id}] (Graph|1) [æŒä¹…åŒ–] æª¢æ¸¬åˆ°ç‹€æ…‹è®Šæ›´ï¼Œæ­£åœ¨å°‡æ–°çš„ GameState å¯«å…¥è³‡æ–™åº«...")
        gs.viewing_mode = final_viewing_mode
        gs.remote_target_path = final_target_path
        await ai_core.update_and_persist_profile({'game_state': gs.model_dump()})
        logger.info(f"[{user_id}] (Graph|1) [æŒä¹…åŒ–] ç‹€æ…‹å·²æˆåŠŸä¿å­˜ã€‚")
    scene_analysis = SceneAnalysisResult(
        viewing_mode=gs.viewing_mode,
        reasoning=final_reasoning,
        target_location_path=gs.remote_target_path,
        action_summary=user_input
    )
    return {"scene_analysis": scene_analysis, "regeneration_count": 0}

async def retrieve_and_query_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    if state.get('raw_lore_objects') is not None:
        logger.info(f"[{user_id}] (Graph|2) Node: retrieve_and_query -> æª¢æ¸¬åˆ°å·²æ¢å¾©çš„ LORE ä¸Šä¸‹æ–‡ï¼Œå°‡è·³éŽé‡æ–°æŸ¥è©¢ã€‚")
        rag_context_dict = await ai_core.retrieve_and_summarize_memories(user_input)
        return {
            "rag_context": rag_context_dict.get("summary", "ç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚"),
            "raw_lore_objects": state['raw_lore_objects'],
            "sanitized_query_for_tools": user_input,
            "last_response_text": state.get('last_response_text')
        }
    logger.info(f"[{user_id}] (Graph|2) Node: retrieve_and_query -> æ­£åœ¨åŸ·è¡Œ RAG æª¢ç´¢...")
    sanitized_query = user_input
    try:
        literary_chain_prompt = ai_core.get_literary_euphemization_chain()
        full_prompt = ai_core._safe_format_prompt(literary_chain_prompt, {"dialogue_history": user_input})
        result = await ai_core.ainvoke_with_rotation(
            full_prompt, 
            retry_strategy='euphemize'
        )
        if result:
            sanitized_query = result
    except Exception as e:
        logger.warning(f"[{user_id}] (Graph|2) æºé ­æ¸…æ´—å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŽŸå§‹è¼¸å…¥é€²è¡ŒæŸ¥è©¢ã€‚è©³ç´°éŒ¯èª¤: {type(e).__name__}")
    rag_context_dict = await ai_core.retrieve_and_summarize_memories(sanitized_query)
    rag_context_str = rag_context_dict.get("summary", "æ²’æœ‰æª¢ç´¢åˆ°ç›¸é—œçš„é•·æœŸè¨˜æ†¶ã€‚")
    all_lores = await lore_book.get_all_lores_for_user(user_id)
    logger.info(f"[{user_id}] (Graph|2) RAG æª¢ç´¢å®Œæˆã€‚")
    return {
        "rag_context": rag_context_str,
        "raw_lore_objects": [], 
        "sanitized_query_for_tools": sanitized_query
    }

async def expansion_decision_and_execution_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    safe_query_text = state['sanitized_query_for_tools']
    raw_lore_objects = state.get('raw_lore_objects', [])
    logger.info(f"[{user_id}] (Graph|3) Node: expansion_decision_and_execution -> æ­£åœ¨æ±ºç­–æ˜¯å¦æ“´å±•LORE...")
    all_lores = await lore_book.get_all_lores_for_user(user_id)
    lightweight_lore_json = json.dumps(
        [{"name": lore.structured_content.get("name"), "description": (lore.narrative_content or "")[:50]} for lore in all_lores if lore.structured_content],
        ensure_ascii=False
    )
    decision_chain_prompt = ai_core.get_expansion_decision_chain()
    full_prompt = ai_core._safe_format_prompt(decision_chain_prompt, {
        "user_input": safe_query_text, 
        "existing_characters_json": lightweight_lore_json
    })
    decision = await ai_core.ainvoke_with_rotation(
        full_prompt, 
        output_schema=ExpansionDecision,
        retry_strategy='euphemize'
    )
    if not decision or not decision.should_expand:
        reason = decision.reasoning if decision else "æ±ºç­–éˆå¤±æ•—"
        logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæžœï¼šç„¡éœ€æ“´å±•ã€‚ç†ç”±: {reason}")
        return {"planning_subjects": [lore.structured_content for lore in all_lores if lore.structured_content]}
    logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæžœï¼šéœ€è¦æ“´å±•ã€‚ç†ç”±: {decision.reasoning}ã€‚æ­£åœ¨åŸ·è¡ŒLOREæ“´å±•...")
    newly_created_lores = []
    try:
        expansion_prompt_template = ai_core.get_lore_expansion_pipeline_prompt()
        existing_lore_names = [lore.structured_content.get("name") for lore in all_lores if lore.structured_content and lore.structured_content.get("name")]
        expansion_prompt = ai_core._safe_format_prompt(expansion_prompt_template, {
            "user_input": safe_query_text,
            "existing_lore_json": json.dumps(existing_lore_names, ensure_ascii=False)
        })
        from .schemas import CanonParsingResult
        expansion_result = await ai_core.ainvoke_with_rotation(
            expansion_prompt,
            output_schema=CanonParsingResult,
            retry_strategy='euphemize'
        )
        if expansion_result:
            newly_created_lores = await ai_core.parse_and_create_lore_from_canon_object(expansion_result)
            created_names = [lore.structured_content.get("name", "æœªçŸ¥") for lore in newly_created_lores if lore.structured_content]
            logger.info(f"[{user_id}] (Graph|3) æ“´å±•æˆåŠŸï¼Œå‰µå»ºäº† {len(created_names)} å€‹æ–°å¯¦é«”éª¨æž¶: {created_names}")
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|3) LOREæ“´å±•åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    final_lore_objects = all_lores + newly_created_lores
    final_lores_map = {lore.key: lore for lore in final_lore_objects}
    return {"planning_subjects": [lore.structured_content for lore in final_lores_map.values() if lore.structured_content]}

async def preemptive_tool_call_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    logger.info(f"[{user_id}] (Graph|4) Node: preemptive_tool_call -> æ­£åœ¨è§£æžå‰ç½®å·¥å…·èª¿ç”¨...")
    tool_parsing_chain_prompt = ai_core.get_preemptive_tool_parsing_chain()
    character_list_str = ", ".join([ps.get("name", "") for ps in state.get("planning_subjects", []) if ps and ps.get("name")])
    full_prompt = ai_core._safe_format_prompt(tool_parsing_chain_prompt, {
        "user_input": user_input, 
        "character_list_str": character_list_str
    })
    from .schemas import ToolCallPlan
    tool_call_plan = await ai_core.ainvoke_with_rotation(
        full_prompt,
        output_schema=ToolCallPlan,
        retry_strategy='euphemize'
    )
    if not tool_call_plan or not tool_call_plan.plan:
        logger.info(f"[{user_id}] (Graph|4) æœªè§£æžåˆ°æ˜Žç¢ºçš„å·¥å…·èª¿ç”¨ã€‚")
        return {"tool_results": "ç³»çµ±äº‹ä»¶ï¼šç„¡å‰ç½®å·¥å…·è¢«èª¿ç”¨ã€‚"}
    logger.info(f"[{user_id}] (Graph|4) è§£æžåˆ° {len(tool_call_plan.plan)} å€‹å·¥å…·èª¿ç”¨ï¼Œæº–å‚™åŸ·è¡Œ...")
    tool_context.set_context(user_id, ai_core)
    results_summary = ""
    try:
        if not ai_core.profile:
             raise Exception("AI Profileå°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•ç²å–ç•¶å‰åœ°é»žã€‚")
        current_location = ai_core.profile.game_state.location_path
        results_summary, _ = await ai_core._execute_tool_call_plan(tool_call_plan, current_location)
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        results_summary = f"ç³»çµ±äº‹ä»¶ï¼šå·¥å…·åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"
    finally:
        tool_context.set_context(None, None)
    logger.info(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œå®Œç•¢ã€‚")
    return {"tool_results": results_summary}
    
async def assemble_world_snapshot_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Graph|5) Node: assemble_world_snapshot -> æ­£åœ¨çµ„è£ã€ç´”æ·¨ç‰ˆã€‘ä¸Šä¸‹æ–‡å¿«ç…§...")
    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|5) è‡´å‘½éŒ¯èª¤: ai_core.profile æœªåŠ è¼‰ï¼")
        return {"world_snapshot": "éŒ¯èª¤ï¼šAI Profile ä¸Ÿå¤±ã€‚"}
    gs = ai_core.profile.game_state
    context_vars = {
        'username': ai_core.profile.user_profile.name,
        'ai_name': ai_core.profile.ai_profile.name,
        'world_settings': ai_core.profile.world_settings or "æœªè¨­å®š",
        'ai_settings': ai_core.profile.ai_profile.description or "æœªè¨­å®š",
        'retrieved_context': state.get('rag_context', "ç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚"),
        'possessions_context': f"åœ˜éšŠåº«å­˜: {', '.join(gs.inventory) or 'ç©ºçš„'}",
        'quests_context': "ç•¶å‰ç„¡ä»»å‹™ã€‚",
        'location_context': f"ç•¶å‰åœ°é»ž: {' > '.join(gs.location_path)}",
        'npc_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        'relevant_npc_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        'explicit_character_files_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰",
        'player_location': " > ".join(gs.location_path),
        'viewing_mode': gs.viewing_mode,
        'remote_target_path_str': " > ".join(gs.remote_target_path) if gs.remote_target_path else "æœªæŒ‡å®š",
        'scene_rules_context': "ï¼ˆä¸Šä¸‹æ–‡å·²ç”±RAGæä¾›ï¼‰"
    }
    if not ai_core.world_snapshot_template:
        logger.error(f"[{user_id}] (Graph|5) è‡´å‘½éŒ¯èª¤: world_snapshot_template æœªåŠ è¼‰ï¼")
        return {"world_snapshot": "éŒ¯èª¤ï¼šä¸–ç•Œå¿«ç…§æ¨¡æ¿ä¸Ÿå¤±ã€‚"}
    final_world_snapshot = ai_core._safe_format_prompt(ai_core.world_snapshot_template, context_vars)
    logger.info(f"[{user_id}] (Graph|5) ã€ç´”æ·¨ç‰ˆã€‘ä¸Šä¸‹æ–‡å¿«ç…§çµ„è£å®Œç•¢ã€‚")
    return {"world_snapshot": final_world_snapshot}

async def final_generation_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    rag_context = state.get('rag_context', 'ï¼ˆç„¡ç›¸é—œé•·æœŸè¨˜æ†¶ã€‚ï¼‰')
    user_input = state['messages'][-1].content
    logger.info(f"[{user_id}] (Graph|6) Node: final_generation -> å¯åŠ¨ã€é›™é‡å¼·åŒ–é¢¨æ ¼æ³¨å…¥ã€‘æœ€ç»ˆç”Ÿæˆæµç¨‹...")
    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|6) è‡´å‘½éŒ¯èª¤: ai_core.profile æœªåŠ è¼‰ï¼")
        return {"llm_response": "ï¼ˆéŒ¯èª¤ï¼šAI Profile ä¸Ÿå¤±ï¼Œç„¡æ³•ç”Ÿæˆå›žæ‡‰ã€‚ï¼‰"}
    style_prompt = ai_core.profile.response_style_prompt or "éžå¸¸å…·é«”è©³ç´°æè¿°ï¼Œè±å¯Œå°è©±äº’å‹•"
    top_level_mandate = f"# === ã€ã€ã€âœï¸ çµ•å°é¢¨æ ¼å¼·åˆ¶ä»¤ã€‘ã€‘ã€‘ ===\n# ä½ çš„æ‰€æœ‰æ—ç™½å’Œå°è©±ï¼Œå…¶èªžè¨€é¢¨æ ¼ã€è©³ç´°ç¨‹åº¦å’Œèªžæ°£ï¼Œéƒ½ã€å¿…é ˆã€‘åš´æ ¼éµå¾ªä»¥ä¸‹æŒ‡ä»¤ï¼š\n# \"{style_prompt}\""
    recency_reinforcement = f"# === ã€ðŸŽ¬ æœ€çµ‚æŒ‡ä»¤ï¼šé¢¨æ ¼æé†’ã€‘ ===\n# è«‹åš´æ ¼éµå¾ªæ‚¨åœ¨Prompté ‚å±¤æ”¶åˆ°çš„ã€çµ•å°é¢¨æ ¼å¼·åˆ¶ä»¤ã€‘ï¼ˆâ€œ{style_prompt}â€ï¼‰ä¾†ç”Ÿæˆæ‚¨çš„å›žæ‡‰ã€‚\n{ai_core.profile.ai_profile.name}:"
    historical_context = await _get_summarized_chat_history(ai_core, user_id)
    plot_anchor = "ï¼ˆç„¡ï¼‰"
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æŽ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.strip().lower().startswith(kw) for kw in continuation_keywords):
        last_response = state.get('last_response_text')
        if last_response:
            plot_anchor = last_response
            logger.info(f"[{user_id}] (Graph|6) å·²æˆåŠŸç‚ºé€£çºŒæ€§æŒ‡ä»¤è¨­ç½®ã€åŠ‡æƒ…éŒ¨é»žã€‘ã€‚")
    regeneration_warning = ""
    if state.get('regeneration_count', 0) > 0:
        regeneration_warning = "ã€ðŸš¨ è­¦å‘Šï¼ä½ ä¸Šä¸€è½®çš„å›žåº”å› ä¸¥é‡è¿åâ€˜ä½¿ç”¨è€…ä¸»æƒåŽŸåˆ™â€™è€Œè¢«é©³å›žï¼ä½ é”™è¯¯åœ°æ‰®æ¼”äº†ä½¿ç”¨è€…ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡é‡æ–°ç”Ÿæˆä½ çš„å›žåº”ï¼Œå¹¶ç¡®ä¿ä½ çš„æ‰€æœ‰æè¿°éƒ½åªé’ˆå¯¹AIè§’è‰²è‡ªèº«ã€‚ã€‘\n\n"
    final_prompt_template = """{regeneration_warning}{core_protocol_prompt}
{top_level_style_mandate}

# === æƒ…å ±ç°¡å ± (ä¾†è‡ª RAG æª¢ç´¢) ===
{rag_context}
# === æƒ…å ±çµæŸ ===

# === åŠ‡æƒ…éŒ¨é»ž (ä¸Šä¸€å¹•çš„æœ€å¾Œå ´æ™¯) ===
{plot_anchor}
# === éŒ¨é»žçµæŸ ===

# === æœ€è¿‘å°è©±æ­·å² ===
{historical_context}
# === æ­·å²çµæŸ ===

# === æœ¬å›žåˆäº’å‹• ===
{username}: {latest_user_input}
{recency_style_reinforcement}"""
    full_prompt = ai_core._safe_format_prompt(
        final_prompt_template,
        {
            "regeneration_warning": regeneration_warning,
            "core_protocol_prompt": ai_core.core_protocol_prompt,
            "top_level_style_mandate": top_level_mandate,
            "rag_context": rag_context,
            "plot_anchor": plot_anchor,
            "historical_context": historical_context,
            "username": ai_core.profile.user_profile.name,
            "latest_user_input": user_input,
            "recency_style_reinforcement": recency_reinforcement
        }
    )
    logger.info(f"[{user_id}] (Graph|6) [æ°¸ä¹…æ€§è½Ÿç‚¸] å·²å•Ÿç”¨ 'force' å‚™æ´ç­–ç•¥ä»¥æœ€å¤§åŒ–æˆåŠŸçŽ‡ã€‚")
    final_response = await ai_core.ainvoke_with_rotation(
        full_prompt,
        retry_strategy='force',
        use_degradation=True
    )
    if not final_response or not final_response.strip():
        logger.critical(f"[{user_id}] (Graph|6) æ ¸å¿ƒç”Ÿæˆé“¾åœ¨æ‰€æœ‰ç­–ç•¥ä¹‹å¾Œæœ€çµ‚å¤±æ•—ï¼")
        final_response = "ï¼ˆæŠ±æ­‰ï¼Œæˆ‘å¥½åƒçªç„¶æ–­çº¿äº†ï¼Œè„‘æµ·ä¸­ä¸€ç‰‡ç©ºç™½...ï¼‰"
    logger.info(f"[{user_id}] (Graph|6) æœ€ç»ˆç”Ÿæˆæµç¨‹å®Œæˆã€‚")
    return {"llm_response": final_response, "regeneration_count": state.get('regeneration_count', 0) + 1}

async def violation_check_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    llm_response = state['llm_response']
    logger.info(f"[{user_id}] (Graph|7) Node: violation_check -> æ­£åœ¨å¯¹AIå›žåº”è¿›è¡Œâ€œä½¿ç”¨è€…ä¸»æƒâ€è¿è§„å®¡æŸ¥...")
    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|7) è‡´å‘½é”™è¯¯: ai_core.profile æœªåŠ è½½ï¼")
        return {"llm_response": llm_response}
    try:
        check_prompt_template = ai_core.get_violation_check_prompt()
        full_prompt = ai_core._safe_format_prompt(
            check_prompt_template,
            {
                "username": ai_core.profile.user_profile.name,
                "llm_response": llm_response
            }
        )
        validation_result = await ai_core.ainvoke_with_rotation(
            full_prompt,
            output_schema=ValidationResult,
            models_to_try_override=[FUNCTIONAL_MODEL]
        )
        if validation_result and validation_result.is_violating:
            logger.warning(f"[{user_id}] (Graph|7) ðŸš¨ å®¡æŸ¥å‘çŽ°è¿è§„ï¼AIå›žåº”ä¾µçŠ¯äº†ä½¿ç”¨è€…ä¸»æƒã€‚å°†è§¦å‘é‡ç”Ÿæˆã€‚")
            new_messages = state['messages'] + [SystemMessage(content="VIOLATION_DETECTED")]
            return {"messages": new_messages}
        else:
            logger.info(f"[{user_id}] (Graph|7) âœ… å®¡æŸ¥é€šè¿‡ï¼ŒAIå›žåº”åˆè§„ã€‚")
            new_messages = [m for m in state['messages'] if m.content != "VIOLATION_DETECTED"]
            return {"messages": new_messages, "llm_response": llm_response}
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|7) ðŸ”¥ è¿è§„å®¡æŸ¥èŠ‚ç‚¹æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"llm_response": llm_response}

async def validate_and_persist_node(state: ConversationGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    # [v34.1 æ ¸å¿ƒä¿®æ­£] ç¢ºä¿ user_input ä¾†æºæ­£ç¢º
    user_input_message = next((m for m in reversed(state['messages']) if isinstance(m, HumanMessage)), None)
    if not user_input_message:
        logger.error(f"[{user_id}] (Graph|8) è‡´å‘½é”™è¯¯: åœ¨æ¶ˆæ¯åˆ—è¡¨ä¸­æ‰¾ä¸åˆ° HumanMessageï¼Œæ— æ³•æŒä¹…åŒ–ã€‚")
        # å³ä½¿å‡ºéŒ¯ï¼Œä¹Ÿè¦è¿”å›ž final_output ä»¥å…å‰ç«¯å¡ä½
        return {"final_output": state.get('llm_response', "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ç”¨æˆ¶è¼¸å…¥ã€‚")}
    
    user_input = user_input_message.content
    llm_response = state['llm_response']
    logger.info(f"[{user_id}] (Graph|8) Node: validate_and_persist -> æ­£åœ¨é©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–...")
    clean_response = llm_response.strip()
    snapshot_for_analysis = {
        "user_input": user_input,
        "final_response": clean_response,
    }
    asyncio.create_task(ai_core._background_lore_extraction(snapshot_for_analysis))
    await ai_core._save_interaction_to_dbs(f"ä½¿ç”¨è€…: {user_input}\n\nAI:\n{clean_response}")
    logger.info(f"[{user_id}] (Graph|8) å°è©±æ­·å²å·²æ›´æ–°ä¸¦é€²è¡Œé›™é‡æŒä¹…åŒ–ã€‚")
    snapshot_for_next_turn = {
        "raw_lore_objects": state.get("raw_lore_objects", []),
        "last_response_text": clean_response
    }
    ai_core.last_context_snapshot = snapshot_for_next_turn
    logger.info(f"[{user_id}] (Graph|8) å·²ç‚ºä¸‹ä¸€è¼ªå‰µå»ºä¸Šä¸‹æ–‡å¿«ç…§ã€‚")
    logger.info(f"[{user_id}] (Graph|8) ç‹€æ…‹æŒä¹…åŒ–å®Œæˆã€‚")
    return {"final_output": clean_response}
    
async def _get_summarized_chat_history(ai_core: AILover, user_id: str, num_messages: int = 8) -> str:
    if not ai_core.profile: return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
    scene_key = ai_core._get_scene_key()
    chat_history_manager = ai_core.scene_histories.get(scene_key, ChatMessageHistory())
    if not chat_history_manager.messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
    recent_messages = chat_history_manager.messages[-num_messages:]
    if not recent_messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
    raw_history_text = "\n".join([f"{'ä½¿ç”¨è€…' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in recent_messages])
    try:
        literary_chain_prompt = ai_core.get_literary_euphemization_chain()
        summary = await ai_core.ainvoke_with_rotation(literary_chain_prompt, {"dialogue_history": raw_history_text}, retry_strategy='euphemize')
        if not summary or not summary.strip():
            raise Exception("Summarization returned empty content.")
        return f"ã€æœ€è¿‘å°è©±æ‘˜è¦ã€‘:\n{summary}"
    except Exception as e:
        logger.error(f"[{user_id}] (History Summarizer) ç”Ÿæˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚è¿”å›žä¸­æ€§æç¤ºã€‚")
        return "ï¼ˆæ­·å²å°è©±æ‘˜è¦å› é”™è¯¯è€Œç”Ÿæˆå¤±è´¥ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡å¯èƒ½ç¼ºå¤±ã€‚ï¼‰"

def should_regenerate(state: ConversationGraphState) -> Literal["final_generation", "validate_and_persist"]:
    MAX_REGENERATIONS = 2
    user_id = state['user_id']
    if state['messages'] and state['messages'][-1].content == "VIOLATION_DETECTED":
        count = state.get('regeneration_count', 0)
        if count < MAX_REGENERATIONS:
            logger.warning(f"[{user_id}] [Graph Control] è§¦å‘é‡ç”Ÿæˆ (å°è¯• {count + 1}/{MAX_REGENERATIONS})ã€‚")
            return "final_generation"
        else:
            logger.error(f"[{user_id}] [Graph Control] ðŸ”¥ å·²è¾¾åˆ°æœ€å¤§é‡ç”Ÿæˆæ¬¡æ•° ({MAX_REGENERATIONS})ï¼å°†å¼ºåˆ¶é€šè¿‡è¿è§„å›žåº”ä»¥é¿å…æ­»å¾ªçŽ¯ã€‚")
            return "validate_and_persist"
    return "validate_and_persist"

def create_main_response_graph() -> StateGraph:
    """åˆ›å»ºå¹¶è¿žæŽ¥æ‰€æœ‰èŠ‚ç‚¹ï¼Œæž„å»ºåŒ…å«è¿è§„å®¡æŸ¥å¾ªçŽ¯çš„æœ€ç»ˆå¯¹è¯å›¾ã€‚"""
    graph = StateGraph(ConversationGraphState)
    
    graph.add_node("perceive_scene", perceive_scene_node)
    graph.add_node("retrieve_and_query", retrieve_and_query_node)
    graph.add_node("expansion_decision_and_execution", expansion_decision_and_execution_node)
    graph.add_node("preemptive_tool_call", preemptive_tool_call_node)
    graph.add_node("assemble_world_snapshot", assemble_world_snapshot_node)
    graph.add_node("final_generation", final_generation_node)
    graph.add_node("violation_check", violation_check_node)
    graph.add_node("validate_and_persist", validate_and_persist_node)
    
    graph.set_entry_point("perceive_scene")
    
    graph.add_edge("perceive_scene", "retrieve_and_query")
    graph.add_edge("retrieve_and_query", "expansion_decision_and_execution")
    graph.add_edge("expansion_decision_and_execution", "preemptive_tool_call")
    graph.add_edge("preemptive_tool_call", "assemble_world_snapshot")
    graph.add_edge("assemble_world_snapshot", "final_generation")
    
    graph.add_edge("final_generation", "violation_check")
    graph.add_conditional_edges(
        "violation_check",
        should_regenerate,
        {
            "final_generation": "final_generation",
            "validate_and_persist": "validate_and_persist"
        }
    )
    
    graph.add_edge("validate_and_persist", END)
    
    return graph.compile()

# --- Setup Graph (ä¿æŒä¸è®Š) ---
async def process_canon_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state['canon_text']
    logger.info(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> ç¯€é»žå·²å•Ÿå‹•ã€‚")
    try:
        if canon_text:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æª¢æ¸¬åˆ°ä¸–ç•Œè–ç¶“æ–‡æœ¬ (é•·åº¦: {len(canon_text)})ï¼Œé–‹å§‹è™•ç†...")
            await ai_core.parse_and_create_lore_from_canon(canon_text)
            logger.info(f"[{user_id}] (Setup Graph|1/4) LORE æ™ºèƒ½è§£æžå®Œæˆã€‚")
        else:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æœªæä¾›ä¸–ç•Œè–ç¶“æ–‡æœ¬ï¼Œè·³éŽè™•ç†ã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {}

async def complete_profiles_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»žå·²å•Ÿå‹•ï¼Œæº–å‚™è£œå®Œè§’è‰²æª”æ¡ˆ...")
    try:
        await ai_core.complete_character_profiles()
        logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»žåŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {}

async def world_genesis_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state.get('canon_text')
    logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»žå·²å•Ÿå‹•...")
    genesis_result = None
    try:
        await ai_core.generate_world_genesis(canon_text=canon_text)
        logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»žåŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    return {"genesis_result": genesis_result}

async def generate_opening_scene_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state.get('canon_text')
    opening_scene = ""
    logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»žå·²å•Ÿå‹•...")
    try:
        opening_scene = await ai_core.generate_opening_scene(canon_text=canon_text)
        if not opening_scene or not opening_scene.strip():
            opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
        logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»žåŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
    return {"opening_scene": opening_scene}

def create_setup_graph() -> StateGraph:
    graph = StateGraph(SetupGraphState)
    graph.add_node("complete_profiles", complete_profiles_node)
    graph.add_node("generate_opening_scene", generate_opening_scene_node)
    graph.set_entry_point("complete_profiles")
    graph.add_edge("complete_profiles", "generate_opening_scene")
    graph.add_edge("generate_opening_scene", END)
    return graph.compile()
