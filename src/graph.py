# src/graph.py çš„ä¸­æ–‡è¨»é‡‹(v30.0 - ä¿¡æ¯æ³¨å…¥å¼æ¶æ„)
# æ›´æ–°ç´€éŒ„:
# v30.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v22.0 (2025-09-22): [ç½é›£æ€§BUGä¿®å¾©] è§£å†³äº†å› é‡å‘½åæ¸²æŸ“èŠ‚ç‚¹å¯¼è‡´çš„ NameErrorã€‚
# v21.1 (2025-09-10): [ç½é›£æ€§BUGä¿®å¾©] æ¢å¤äº†æ‰€æœ‰è¢«å…ˆå‰ç‰ˆæœ¬é”™è¯¯çœç•¥çš„ `SetupGraph` ç›¸å…³èŠ‚ç‚¹ã€‚

import sys
import asyncio
import json
import re
from typing import Dict, List, Literal, Optional, Any

from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.chat_message_histories import ChatMessageHistory
from langgraph.graph import StateGraph, END

from .ai_core import AILover
from .logger import logger
from .graph_state import ConversationGraphState, SetupGraphState
from . import lore_book, tools
from .schemas import (CharacterProfile, ExpansionDecision, 
                      UserInputAnalysis, SceneAnalysisResult, SceneCastingResult, 
                      WorldGenesisResult, IntentClassificationResult, StyleAnalysisResult,
                      CharacterQuantificationResult, ToolCall)
from .tool_context import tool_context
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- [v30.0 æ–°æ¶æ„] ä¸»å°è©±åœ– (Main Conversation Graph) çš„ç¯€é» ---

# å‡½å¼ï¼š[æ–°] åœºæ™¯æ„ŸçŸ¥èŠ‚ç‚¹
# æ›´æ–°ç´€éŒ„:
# v1.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v2.0 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] å¼•å…¥äº†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¦–è§’ä¿æŒã€‘ç­–ç•¥ï¼Œä»¥è§£æ±ºåœ¨é€£çºŒæ€§æŒ‡ä»¤ï¼ˆå¦‚â€œç»§ç»­â€ï¼‰ä¸‹ï¼Œå ´æ™¯è¦–è§’è¢«éŒ¯èª¤é‡ç½®çš„å•é¡Œã€‚
async def perceive_scene_node(state: ConversationGraphState) -> Dict:
    """[1] åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæ¨æ–­ç›®æ ‡åœ°ç‚¹å’Œè§†è§’æ¨¡å¼ï¼ˆè¿‘æ™¯/è¿œæ™¯ï¼‰ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content.strip()
    logger.info(f"[{user_id}] (Graph|1) Node: perceive_scene -> æ­£åœ¨æ„ŸçŸ¥åœºæ™¯...")

    if not ai_core.profile:
        logger.error(f"[{user_id}] (Graph|1) ai_core.profile æœªåŠ è½½ï¼Œæ— æ³•æ„ŸçŸ¥åœºæ™¯ã€‚")
        return {"scene_analysis": SceneAnalysisResult(viewing_mode='local', reasoning='é”™è¯¯ï¼šAI profile æœªåŠ è½½ã€‚', action_summary=user_input)}

    gs = ai_core.profile.game_state
    
    # [v2.0 æ ¸å¿ƒä¿®æ­£] æ­¥é©Ÿ 1: è™•ç†é€£çºŒæ€§æŒ‡ä»¤
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.lower().startswith(kw) for kw in continuation_keywords):
        logger.info(f"[{user_id}] (Graph|1) æª¢æ¸¬åˆ°é€£çºŒæ€§æŒ‡ä»¤ï¼Œå°‡ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ã€‚")
        # ç›´æ¥ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„ç‹€æ…‹ï¼Œç„¡éœ€ä»»ä½•åˆ†æ
        scene_analysis = SceneAnalysisResult(
            viewing_mode=gs.viewing_mode,
            reasoning="ç¹¼æ‰¿ä¸Šä¸€è¼ªçš„å ´æ™¯ç‹€æ…‹ã€‚",
            target_location_path=gs.remote_target_path,
            action_summary=user_input
        )
        return {"scene_analysis": scene_analysis}

    # [v2.0 æ ¸å¿ƒä¿®æ­£] æ­¥é©Ÿ 2: è™•ç†æ¨¡å¼åˆ‡æ›å’Œç›®æ¨™æ›´æ–°
    new_viewing_mode = 'local'
    new_target_path = None

    # å˜—è©¦ä½¿ç”¨ LLM é€²è¡Œæ™ºèƒ½æ¨æ–·
    location_chain = ai_core.get_contextual_location_chain()
    scene_context_lores = [lore.content for lore in state.get('raw_lore_objects_for_view_decision', []) if lore.category == 'npc_profile']
    scene_context_json_str = json.dumps(scene_context_lores, ensure_ascii=False, indent=2)

    location_result = await ai_core.ainvoke_with_rotation(
        location_chain, 
        {"user_input": user_input, "world_settings": ai_core.profile.world_settings or "æœªè®¾å®š", "scene_context_json": scene_context_json_str},
        retry_strategy='euphemize'
    )

    if location_result and location_result.location_path:
        logger.info(f"[{user_id}] (Graph|1) LLM æ„ŸçŸ¥æˆåŠŸã€‚æ¨æ–·å‡ºçš„ç›®æ¨™åœ°é»: {location_result.location_path}")
        new_target_path = location_result.location_path
        new_viewing_mode = 'remote'
    
    # [v2.0 æ ¸å¿ƒä¿®æ­£] æ­¥é©Ÿ 3: æ‡‰ç”¨ã€Œé ç¨‹å„ªå…ˆã€çš„ç‹€æ…‹ä¿æŒé‚è¼¯
    final_viewing_mode = gs.viewing_mode
    final_target_path = gs.remote_target_path

    if gs.viewing_mode == 'remote':
        # ç•¶å‰è™•æ–¼é ç¨‹æ¨¡å¼
        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ˜ç¢ºçš„è¿”å›æœ¬åœ°çš„ä¿¡è™Ÿ
        is_explicit_local_move = any(user_input.startswith(kw) for kw in ["å»", "å‰å¾€", "ç§»å‹•åˆ°", "æ—…è¡Œåˆ°"])
        is_direct_ai_interaction = ai_core.profile.ai_profile.name in user_input
        
        if is_explicit_local_move or is_direct_ai_interaction:
            # ä¿¡è™Ÿæ˜ç¢ºï¼šåˆ‡æ›å›æœ¬åœ°
            final_viewing_mode = 'local'
            final_target_path = None
            logger.info(f"[{user_id}] (Graph|1) æª¢æ¸¬åˆ°æ˜ç¢ºçš„æœ¬åœ°æŒ‡ä»¤ï¼Œå°æ¼”è¦–è§’å¾ 'remote' åˆ‡æ›å› 'local'ã€‚")
        elif new_viewing_mode == 'remote' and new_target_path and new_target_path != gs.remote_target_path:
            # ä¿¡è™Ÿä¸æ˜ç¢ºï¼Œä½†æ¢æ¸¬åˆ°æ–°çš„é ç¨‹ç›®æ¨™ï¼šæ›´æ–°ç›®æ¨™
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) åœ¨é ç¨‹æ¨¡å¼ä¸‹ï¼Œæ›´æ–°äº†è§€å¯Ÿç›®æ¨™åœ°é»ç‚º: {final_target_path}")
        else:
            # ä¿æŒé ç¨‹æ¨¡å¼å’Œç•¶å‰ç›®æ¨™ä¸è®Š
            logger.info(f"[{user_id}] (Graph|1) æœªæª¢æ¸¬åˆ°æœ¬åœ°åˆ‡æ›ä¿¡è™Ÿï¼Œå°æ¼”è¦–è§’ä¿æŒç‚º 'remote'ã€‚")
    else: # gs.viewing_mode == 'local'
        # ç•¶å‰è™•æ–¼æœ¬åœ°æ¨¡å¼ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ›åˆ°é ç¨‹
        if new_viewing_mode == 'remote' and new_target_path:
            final_viewing_mode = 'remote'
            final_target_path = new_target_path
            logger.info(f"[{user_id}] (Graph|1) æª¢æ¸¬åˆ°é ç¨‹æè¿°æŒ‡ä»¤ï¼Œå°æ¼”è¦–è§’å¾ 'local' åˆ‡æ›åˆ° 'remote'ã€‚ç›®æ¨™: {final_target_path}")

    # æ›´æ–°ä¸¦æŒä¹…åŒ–éŠæˆ²ç‹€æ…‹
    if gs.viewing_mode != final_viewing_mode or gs.remote_target_path != final_target_path:
        gs.viewing_mode = final_viewing_mode
        gs.remote_target_path = final_target_path
        await ai_core.update_and_persist_profile({'game_state': gs.model_dump()})
    
    scene_analysis = SceneAnalysisResult(
        viewing_mode=gs.viewing_mode,
        reasoning=f"å ´æ™¯æ„ŸçŸ¥å®Œæˆã€‚",
        target_location_path=gs.remote_target_path,
        action_summary=user_input
    )
    return {"scene_analysis": scene_analysis}
# å‡½å¼ï¼š[æ–°] åœºæ™¯æ„ŸçŸ¥èŠ‚ç‚¹





# å‡½å¼ï¼š[æ–°] è¨˜æ†¶èˆ‡ LORE æŸ¥è©¢ç¯€é»
# æ›´æ–°ç´€éŒ„:
# v2.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v2.1 (2025-10-15): [æ€§èƒ½å„ªåŒ–] æ–°å¢äº†å°çŸ­æŒ‡ä»¤çš„åˆ¤æ–·ï¼Œé¿å…å° "åä¸‹" ç­‰ç°¡å–®æŒ‡ä»¤åŸ·è¡Œä¸å¿…è¦çš„ RAG æª¢ç´¢ï¼Œä»¥ç¯€çœ Embedding API é…é¡ã€‚
# v3.0 (2025-10-15): [åŠŸèƒ½å„ªåŒ–] æ­¤å‡½å¼ç¾åœ¨èª¿ç”¨å…·æœ‰å ´æ™¯æ„ŸçŸ¥èƒ½åŠ›çš„ `_query_lore_from_entities`ï¼ŒæŸ¥è©¢çµæœæ›´ç²¾æº–ã€‚
async def retrieve_and_query_node(state: ConversationGraphState) -> Dict:
    """[2] æ¸…æ´—ä½¿ç”¨è€…è¼¸å…¥ï¼Œæª¢ç´¢ RAG è¨˜æ†¶ï¼Œä¸¦æŸ¥è©¢æ‰€æœ‰ç›¸é—œçš„ LOREã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    scene_analysis = state['scene_analysis']
    logger.info(f"[{user_id}] (Graph|2) Node: retrieve_and_query -> æ­£åœ¨æª¢ç´¢è¨˜æ†¶èˆ‡æŸ¥è©¢LORE...")

    # æºé ­æ¸…æ´—
    sanitized_query = user_input
    try:
        literary_chain = ai_core.get_literary_euphemization_chain()
        result = await ai_core.ainvoke_with_rotation(literary_chain, {"dialogue_history": user_input}, retry_strategy='euphemize')
        if result:
            sanitized_query = result.content if hasattr(result, 'content') else str(result)
    except Exception:
        logger.warning(f"[{user_id}] (Graph|2) æºé ­æ¸…æ´—å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸå§‹è¼¸å…¥é€²è¡ŒæŸ¥è©¢ã€‚")

    # [v2.1 æ ¸å¿ƒä¿®æ­£] RAG æª¢ç´¢å„ªåŒ–
    rag_context_str = "æ²’æœ‰æª¢ç´¢åˆ°ç›¸é—œçš„é•·æœŸè¨˜æ†¶ã€‚"
    if len(user_input) > 10 or any(kw in user_input for kw in ["èª°", "ä»€éº¼", "å›æ†¶", "è¨˜å¾—"]):
        logger.info(f"[{user_id}] (Graph|2) è¼¸å…¥è¼ƒè¤‡é›œï¼ŒåŸ·è¡Œ RAG æª¢ç´¢...")
        rag_context_str = await ai_core.retrieve_and_summarize_memories(sanitized_query)
    else:
        logger.info(f"[{user_id}] (Graph|2) è¼¸å…¥ç‚ºç°¡å–®æŒ‡ä»¤ï¼Œè·³é RAG æª¢ç´¢ä»¥ç¯€çœé…é¡ã€‚")


    # [v3.0 æ ¸å¿ƒä¿®æ­£] LORE æŸ¥è©¢ç¾åœ¨å…·æœ‰å ´æ™¯æ„ŸçŸ¥èƒ½åŠ›
    is_remote = scene_analysis.viewing_mode == 'remote'
    final_lores = await ai_core._query_lore_from_entities(sanitized_query, is_remote_scene=is_remote)
        
    logger.info(f"[{user_id}] (Graph|2) æŸ¥è©¢å®Œæˆã€‚æª¢ç´¢åˆ° {len(final_lores)} æ¢ç›¸é—œLOREã€‚")
    
    return {
        "rag_context": rag_context_str,
        "raw_lore_objects": final_lores,
        "sanitized_query_for_tools": sanitized_query
    }
# å‡½å¼ï¼š[æ–°] è¨˜æ†¶èˆ‡ LORE æŸ¥è©¢ç¯€é»




# å‡½å¼ï¼š[æ–°] LORE æ“´å±•æ±ºç­–èˆ‡åŸ·è¡Œç¯€é»
# æ›´æ–°ç´€éŒ„:
# v3.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v2.0 (2025-10-07): [æ¶æ§‹é‡æ§‹] æ­¤èŠ‚ç‚¹çš„èŒè´£è¢«æ‰©å±•ã€‚å®ƒç°åœ¨è´Ÿè´£ç»„è£…æ‰€æœ‰ä¸åŒæ¥æºçš„ä¸Šä¸‹æ–‡ï¼ˆRAG è®°å¿†ã€çŸ­æœŸå¯¹è¯å†å²ã€ä¸–ç•Œå¿«ç…§ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§â€œå†å² -> äº‹å® -> æŒ‡ä»¤â€çš„é¡ºåºï¼Œå°†å®ƒä»¬å¡«å……åˆ°æ–°çš„æç¤ºè¯æ¨¡æ¿ä¸­ï¼Œç„¶åè°ƒç”¨æ ¸å¿ƒç”Ÿæˆé“¾ã€‚
# v3.1 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] åœ¨èª¿ç”¨ `expansion_decision_chain` æ™‚ï¼Œè£œå…¨äº†ç¼ºå¤±çš„ `username` å’Œ `ai_name` åƒæ•¸ï¼Œè§£æ±ºäº† `KeyError`ã€‚
async def expansion_decision_and_execution_node(state: ConversationGraphState) -> Dict:
    """[3] æ±ºç­–æ˜¯å¦éœ€è¦æ“´å±• LOREï¼Œå¦‚æœéœ€è¦ï¼Œå‰‡ç«‹å³åŸ·è¡Œæ“´å±•ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    safe_query_text = state['sanitized_query_for_tools']
    raw_lore_objects = state.get('raw_lore_objects', [])
    logger.info(f"[{user_id}] (Graph|3) Node: expansion_decision_and_execution -> æ­£åœ¨æ±ºç­–æ˜¯å¦æ“´å±•LORE...")

    # Plan A: å˜—è©¦ä½¿ç”¨ LLM é€²è¡Œæ±ºç­–
    lightweight_lore_json = json.dumps(
        [{"name": lore.content.get("name"), "description": lore.content.get("description")} for lore in raw_lore_objects if lore.category == 'npc_profile'],
        ensure_ascii=False
    )
    decision_chain = ai_core.get_expansion_decision_chain()
    
    # [v3.1 æ ¸å¿ƒä¿®æ­£] æº–å‚™èª¿ç”¨ `expansion_decision_chain` æ‰€éœ€çš„æ‰€æœ‰åƒæ•¸
    decision_params = {
        "user_input": safe_query_text, 
        "existing_characters_json": lightweight_lore_json, 
        "examples": "", # ç¯„ä¾‹ç›®å‰åœ¨æç¤ºè©ä¸­ç¡¬ç·¨ç¢¼ï¼Œæœªä¾†å¯ä»¥å‹•æ…‹æ³¨å…¥
        "username": ai_core.profile.user_profile.name,
        "ai_name": ai_core.profile.ai_profile.name
    }
    decision = await ai_core.ainvoke_with_rotation(
        decision_chain, 
        decision_params,
        retry_strategy='euphemize'
    )

    if not decision:
        # Plan B (å‚™æ´): LLM å¤±æ•—ï¼Œå•Ÿå‹•åŸºæ–¼ LORE è¦†è“‹ç‡çš„å‚™æ´æ±ºç­–
        logger.warning(f"[{user_id}] (Graph|3) LOREæ“´å±•æ±ºç­–éˆå¤±æ•—ï¼Œå•Ÿå‹•ã€åŸºæ–¼LOREè¦†è“‹ç‡çš„å‚™æ´æ±ºç­–ã€‘ã€‚")
        if len(raw_lore_objects) < 3 and len(safe_query_text) > 15:
            decision = ExpansionDecision(should_expand=True, reasoning="å‚™æ´ï¼šå ´æ™¯ä¸­è§’è‰²è¼ƒå°‘ä¸”ä½¿ç”¨è€…è¼¸å…¥è¼ƒé•·ï¼Œå¯èƒ½éœ€è¦æ–°è§’è‰²ã€‚")
        else:
            decision = ExpansionDecision(should_expand=False, reasoning="å‚™æ´ï¼šæ±ºç­–éˆå¤±æ•—ï¼Œé è¨­ä¸æ“´å±•ã€‚")

    if not decision.should_expand:
        logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæœï¼šç„¡éœ€æ“´å±•ã€‚ç†ç”±: {decision.reasoning}")
        return {"planning_subjects": [lore.content for lore in raw_lore_objects]}

    # --- å¦‚æœéœ€è¦æ“´å±•ï¼Œå‰‡åŸ·è¡Œæ“´å±• ---
    logger.info(f"[{user_id}] (Graph|3) æ±ºç­–çµæœï¼šéœ€è¦æ“´å±•ã€‚ç†ç”±: {decision.reasoning}ã€‚æ­£åœ¨åŸ·è¡ŒLOREæ“´å±•...")
    
    # Plan A: å˜—è©¦ä½¿ç”¨ä¸» casting_chain
    try:
        logger.info(f"[{user_id}] (Graph|3) æ“´å±• Plan A: å˜—è©¦ä½¿ç”¨ä¸»é¸è§’éˆ...")
        quantification_chain = ai_core.get_character_quantification_chain()
        quant_result = await ai_core.ainvoke_with_rotation(quantification_chain, {"user_input": safe_query_text}, retry_strategy='euphemize')
        
        if quant_result and quant_result.character_descriptions:
            gs = ai_core.profile.game_state
            effective_location_path = gs.remote_target_path if gs.viewing_mode == 'remote' and gs.remote_target_path else gs.location_path
            
            casting_chain = ai_core.get_scene_casting_chain()
            cast_result = await ai_core.ainvoke_with_rotation(
                casting_chain,
                {"world_settings": ai_core.profile.world_settings or "", "current_location_path": effective_location_path, "character_descriptions_list": quant_result.character_descriptions},
                retry_strategy='euphemize'
            )
            
            if not cast_result: raise Exception("ä¸»é¸è§’éˆè¿”å›ç©ºå€¼")

            created_names = await ai_core._add_cast_to_scene(cast_result)
            logger.info(f"[{user_id}] (Graph|3) æ“´å±• Plan A æˆåŠŸï¼Œå‰µå»ºäº† {len(created_names)} ä½æ–°è§’è‰²ã€‚")
            
            # ç²å–æ›´æ–°å¾Œçš„æ‰€æœ‰ LORE
            all_lores_after_expansion_state = await retrieve_and_query_node(state)
            return {"planning_subjects": [lore.content for lore in all_lores_after_expansion_state.get("raw_lore_objects", [])]}

    except Exception as e:
        # Plan B (å‚™æ´): ä¸»éˆå¤±æ•—ï¼Œå•Ÿå‹• Gemini å­ä»»å‹™éˆå‚™æ´
        logger.warning(f"[{user_id}] (Graph|3) æ“´å±• Plan A å¤±æ•—: {e}ã€‚å•Ÿå‹•ã€Geminiå­ä»»å‹™éˆå‚™æ´ã€‘...")
        newly_created_lores = await ai_core.gemini_subtask_expansion_fallback(safe_query_text)
        if newly_created_lores:
             logger.info(f"[{user_id}] (Graph|3) å­ä»»å‹™éˆå‚™æ´æˆåŠŸï¼Œå‰µå»ºäº† {len(newly_created_lores)} ä½æ–°è§’è‰²ã€‚")
             all_current_lores = state.get('raw_lore_objects', [])
             all_current_lores.extend(newly_created_lores)
             return {"planning_subjects": [lore.content for lore in all_current_lores]}
        else:
             logger.error(f"[{user_id}] (Graph|3) å­ä»»å‹™éˆå‚™æ´æœ€çµ‚å¤±æ•—ã€‚")
             return {"planning_subjects": [lore.content for lore in raw_lore_objects]}
# å‡½å¼ï¼š[æ–°] LORE æ“´å±•æ±ºç­–èˆ‡åŸ·è¡Œç¯€é»




# å‡½å¼ï¼š[æ–°] å‰ç½®å·¥å…·èª¿ç”¨ç¯€é»
# æ›´æ–°ç´€éŒ„:
# v4.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v2.0 (2025-10-07): [æ¶æ§‹é‡æ§‹] æ­¤èŠ‚ç‚¹çš„èŒè´£è¢«æ‰©å±•ã€‚å®ƒç°åœ¨è´Ÿè´£ç»„è£…æ‰€æœ‰ä¸åŒæ¥æºçš„ä¸Šä¸‹æ–‡ï¼ˆRAG è®°å¿†ã€çŸ­æœŸå¯¹è¯å†å²ã€ä¸–ç•Œå¿«ç…§ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§â€œå†å² -> äº‹å® -> æŒ‡ä»¤â€çš„é¡ºåºï¼Œå°†å®ƒä»¬å¡«å……åˆ°æ–°çš„æç¤ºè¯æ¨¡æ¿ä¸­ï¼Œç„¶åè°ƒç”¨æ ¸å¿ƒç”Ÿæˆé“¾ã€‚
# v2.1 (2025-10-14): [ç½é›£æ€§BUGä¿®å¾©] ä¿®æ­£äº† `CharacterAction` é©—è­‰éŒ¯èª¤ï¼Œç‚ºç³»çµ±è§’è‰²æ·»åŠ äº†é»˜èªçš„ `action_description`ã€‚
async def preemptive_tool_call_node(state: ConversationGraphState) -> Dict:
    """[4] (å…¨æ–°) åˆ¤æ–·ä¸¦åŸ·è¡Œä½¿ç”¨è€…æŒ‡ä»¤ä¸­æ˜ç¢ºçš„ã€éœ€è¦æ”¹è®Šä¸–ç•Œç‹€æ…‹çš„å‹•ä½œã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    logger.info(f"[{user_id}] (Graph|4) Node: preemptive_tool_call -> æ­£åœ¨è§£æå‰ç½®å·¥å…·èª¿ç”¨...")

    # Plan A: å˜—è©¦ä½¿ç”¨ LLM è§£æå·¥å…·èª¿ç”¨
    tool_parsing_chain = ai_core.get_preemptive_tool_parsing_chain()
    # ç¢ºä¿ ai_name ä½œç‚º partial_variables å‚³å…¥
    tool_call_plan = await ai_core.ainvoke_with_rotation(
        tool_parsing_chain,
        {"user_input": user_input, "character_list_str": ", ".join([ps.get("name", "") for ps in state.get("planning_subjects", [])])},
        retry_strategy='euphemize'
    )
    
    # ç°¡å–®çš„å‚™æ´ï¼šå¦‚æœ LLM ç„¡æ³•è§£æï¼Œå°±èªç‚ºæ²’æœ‰å·¥å…·èª¿ç”¨
    if not tool_call_plan or not tool_call_plan.plan:
        logger.info(f"[{user_id}] (Graph|4) æœªè§£æåˆ°æ˜ç¢ºçš„å·¥å…·èª¿ç”¨ã€‚")
        return {"tool_results": "ç³»çµ±äº‹ä»¶ï¼šç„¡å‰ç½®å·¥å…·è¢«èª¿ç”¨ã€‚"}

    logger.info(f"[{user_id}] (Graph|4) è§£æåˆ° {len(tool_call_plan.plan)} å€‹å·¥å…·èª¿ç”¨ï¼Œæº–å‚™åŸ·è¡Œ...")
    
    # åŸ·è¡Œå·¥å…·
    tool_context.set_context(user_id, ai_core)
    try:
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„ TurnPlanï¼Œåªç”¨æ–¼å·¥å…·åŸ·è¡Œ
        from .schemas import TurnPlan, CharacterAction
        # [v2.1 æ ¸å¿ƒä¿®æ­£] ç‚º CharacterAction æ·»åŠ ä¸€å€‹é»˜èªçš„ action_description
        simple_turn_plan = TurnPlan(
            character_actions=[
                CharacterAction(
                    character_name="system", 
                    reasoning="preemptive tool execution", 
                    tool_call=call,
                    action_description=f"åŸ·è¡Œå·¥å…· {call.tool_name}." # æä¾›ä¸€å€‹é»˜èªæè¿°
                ) 
                for call in tool_call_plan.plan
            ]
        )
        results_summary = await ai_core._execute_planned_actions(simple_turn_plan)
    except Exception as e:
        logger.error(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        results_summary = f"ç³»çµ±äº‹ä»¶ï¼šå·¥å…·åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}"
    finally:
        tool_context.set_context(None, None)
    
    logger.info(f"[{user_id}] (Graph|4) å‰ç½®å·¥å…·åŸ·è¡Œå®Œç•¢ã€‚")
    return {"tool_results": results_summary}
# å‡½å¼ï¼š[æ–°] å‰ç½®å·¥å…·èª¿ç”¨ç¯€é»

# å‡½å¼ï¼š[æ–°] ä¸–ç•Œå¿«ç…§çµ„è£ç¯€é» (v2.0 - è·è²¬ç°¡åŒ–)
# æ›´æ–°ç´€éŒ„:
# v2.0 (2025-10-07): [æ¶æ§‹é‡æ§‹] æ ¹æ®æ–°çš„ä¿¡æ¯é¡ºåºè¦æ±‚ï¼Œæ­¤èŠ‚ç‚¹çš„èŒè´£è¢«ç®€åŒ–ã€‚å®ƒç°åœ¨åªè´Ÿè´£å°†ã€å½“å‰åœºæ™¯ã€‘çš„æ‰€æœ‰äº‹å®ï¼ˆLOREã€æ¸¸æˆçŠ¶æ€ã€å·¥å…·ç»“æœï¼‰æ ¼å¼åŒ–ä¸º world_snapshot å­—ç¬¦ä¸²ï¼Œä¸å†å¤„ç†å†å²ä¸Šä¸‹æ–‡ã€‚
# v2.1 (2025-10-14): [ç½é›£æ€§BUGä¿®å¾©] å¢åŠ äº† `username` å’Œ `ai_name` åˆ° `context_vars`ï¼Œè§£æ±º `KeyError`ã€‚
async def assemble_world_snapshot_node(state: ConversationGraphState) -> Dict:
    """[5] (æ ¸å¿ƒ) åŒ¯é›†æ‰€æœ‰ã€ç•¶å‰å ´æ™¯ã€‘çš„ä¿¡æ¯ï¼Œä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–æˆ world_snapshot å­—ç¬¦ä¸²ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Graph|5) Node: assemble_world_snapshot -> æ­£åœ¨çµ„è£ã€ç•¶å‰å ´æ™¯äº‹å¯¦ã€‘...")
    
    planning_subjects = state.get("planning_subjects", [])
    tool_results = state.get("tool_results", "")
    
    npc_context_str = "\n".join([f"- **{npc.get('name', 'æœªçŸ¥NPC')}**: {npc.get('description', 'ç„¡æè¿°')}" for npc in planning_subjects])
    if not npc_context_str: npc_context_str = "ç•¶å‰å ´æ™¯æ²’æœ‰å·²çŸ¥çš„ç‰¹å®šè§’è‰²ã€‚"

    # å°‡å·¥å…·çµæœä¹ŸåŠ å…¥åˆ°å ´æ™¯äº‹å¯¦ä¸­
    if tool_results and "ç„¡å‰ç½®å·¥å…·è¢«èª¿ç”¨" not in tool_results:
        npc_context_str += f"\n\n--- æœ¬å›åˆå³æ™‚äº‹ä»¶ ---\n{tool_results}"

    gs = ai_core.profile.game_state
    
    # [v2.1 æ ¸å¿ƒä¿®æ­£] å¢åŠ  username å’Œ ai_name
    context_vars = {
        'username': ai_core.profile.user_profile.name, # æ–°å¢
        'ai_name': ai_core.profile.ai_profile.name,     # æ–°å¢
        'world_settings': ai_core.profile.world_settings or "æœªè¨­å®š",
        'ai_settings': ai_core.profile.ai_profile.description or "æœªè¨­å®š",
        'retrieved_context': "ï¼ˆæ­¤éƒ¨åˆ†å·²ç§»è‡³æ­·å²ä¸Šä¸‹æ–‡ä¸­å–®ç¨è™•ç†ï¼‰",
        'possessions_context': f"åœ˜éšŠåº«å­˜: {', '.join(gs.inventory) or 'ç©ºçš„'}",
        'quests_context': "ç•¶å‰ç„¡ä»»å‹™ã€‚",
        'location_context': f"ç•¶å‰åœ°é»: {' > '.join(gs.location_path)}",
        'npc_context': npc_context_str,
        'relevant_npc_context': "è«‹åƒè€ƒä¸Šæ–¹åœ¨å ´è§’è‰²åˆ—è¡¨ã€‚",
        'player_location': " > ".join(gs.location_path),
        'viewing_mode': gs.viewing_mode,
        'remote_target_path_str': " > ".join(gs.remote_target_path) if gs.remote_target_path else "æœªæŒ‡å®š",
    }
    
    final_world_snapshot = ai_core.world_snapshot_template.format(**context_vars)
    
    logger.info(f"[{user_id}] (Graph|5) ã€ç•¶å‰å ´æ™¯äº‹å¯¦ã€‘çµ„è£å®Œç•¢ã€‚")
    return {"world_snapshot": final_world_snapshot}
# å‡½å¼ï¼š[æ–°] ä¸–ç•Œå¿«ç…§çµ„è£ç¯€é» (v2.0 - è·è²¬ç°¡åŒ–)





# å‡½å¼ï¼š[æ–°] æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ (v2.0 - ä¼˜åŒ–ä¿¡æ¯é¡ºåº)
# æ›´æ–°çºªå½•:
# v2.0 (2025-10-07): [æ¶æ§‹é‡æ§‹] æ­¤èŠ‚ç‚¹çš„èŒè´£è¢«æ‰©å±•ã€‚å®ƒç°åœ¨è´Ÿè´£ç»„è£…æ‰€æœ‰ä¸åŒæ¥æºçš„ä¸Šä¸‹æ–‡ï¼ˆRAG è®°å¿†ã€çŸ­æœŸå¯¹è¯å†å²ã€ä¸–ç•Œå¿«ç…§ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§â€œå†å² -> äº‹å® -> æŒ‡ä»¤â€çš„é¡ºåºï¼Œå°†å®ƒä»¬å¡«å……åˆ°æ–°çš„æç¤ºè¯æ¨¡æ¿ä¸­ï¼Œç„¶åè°ƒç”¨æ ¸å¿ƒç”Ÿæˆé“¾ã€‚
# v3.0 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] å¼•å…¥äº†ã€æŒ‡ä»¤é˜²ç«ç‰†ã€‘ç­–ç•¥ã€‚é‡æ§‹äº† `human_prompt_template`ï¼Œå°‡ç”¨æˆ¶æŒ‡ä»¤èˆ‡ä¸Šä¸‹æ–‡æ•¸æ“šæ˜ç¢ºåˆ†é›¢ï¼Œä¸¦ç”¨å¼·çƒˆçš„æç¤ºè©å‘½ä»¤ AI çµ•å°å„ªå…ˆåŸ·è¡ŒæŒ‡ä»¤ï¼Œä»¥è§£æ±º AI å› ä¸Šä¸‹æ–‡æ•¸æ“šæ±¡æŸ“è€Œå¿½ç•¥ç”¨æˆ¶æŒ‡ä»¤çš„å•é¡Œã€‚
# v4.0 (2025-10-15): [å¥å£¯æ€§] å¼•å…¥äº†ã€ç„¡æä¸Šä¸‹æ–‡é€šé“ã€‘ï¼Œåœ¨è™•ç†é€£çºŒæ€§æŒ‡ä»¤æ™‚ï¼Œå°‡ä¸Šä¸€è¼ªçš„åŸæ–‡æ³¨å…¥æç¤ºè©ï¼Œç¢ºä¿åŠ‡æƒ…é€£è²«ã€‚
async def final_generation_node(state: ConversationGraphState) -> Dict:
    """[6] (å…¨æ–°) ç»„è£…æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼Œå¹¶è°ƒç”¨ç»Ÿä¸€ç”Ÿæˆé“¾æ¥ä¸€æ­¥åˆ°ä½åœ°åˆ›ä½œå°è¯´ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    world_snapshot = state['world_snapshot']
    user_input = state['messages'][-1].content
    rag_context = state.get('rag_context', 'æ— ç›¸å…³é•¿æœŸè®°å¿†ã€‚')
    logger.info(f"[{user_id}] (Graph|6) Node: final_generation -> å¯åŠ¨æœ€ç»ˆç”Ÿæˆæµç¨‹...")

    # [v4.0 æ ¸å¿ƒä¿®æ­£] æº–å‚™ä¸Šä¸‹æ–‡
    chat_history_str = await _get_summarized_chat_history(ai_core, user_id)
    historical_context = f"{rag_context}\n\n{chat_history_str}"
    
    # [v4.0 æ ¸å¿ƒä¿®æ­£] æª¢æŸ¥æ˜¯å¦ç‚ºé€£çºŒæ€§æŒ‡ä»¤ï¼Œä¸¦æ³¨å…¥ç„¡æä¸Šä¸‹æ–‡
    continuation_keywords = ["ç»§ç»­", "ç¹¼çºŒ", "ç„¶å¾Œå‘¢", "æ¥ä¸‹ä¾†", "go on", "continue"]
    if any(user_input.strip().lower().startswith(kw) for kw in continuation_keywords):
        last_response = state.get('last_response_text')
        if last_response:
            uncompressed_context = (
                f"\n# ==============================================================================\n"
                f"# == ğŸ“ ä¸Šä¸€å¹•å ´æ™¯åŸæ–‡ (çµ•å°åƒè€ƒ) ğŸ“ ==\n"
                f"# ==============================================================================\n"
                f"# èªªæ˜ï¼šé€™æ˜¯ä¸Šä¸€å›åˆçš„å®Œæ•´å ´æ™¯æè¿°ï¼Œä½ çš„ä»»å‹™æ˜¯åŸºæ–¼æ­¤åŸæ–‡ç¹¼çºŒå‰µä½œã€‚\n"
                f"# ------------------------------------------------------------------------------\n"
                f"{last_response}\n"
                f"# =============================================================================="
            )
            # å°‡ç„¡æä¸Šä¸‹æ–‡æ”¾åœ¨æœ€å‰é¢ï¼Œä»¥ç²å¾—æœ€é«˜æ³¨æ„åŠ›æ¬Šé‡
            historical_context = uncompressed_context + "\n\n" + historical_context
            logger.info(f"[{user_id}] (Graph|6) å·²æˆåŠŸç‚ºé€£çºŒæ€§æŒ‡ä»¤æ³¨å…¥ä¸Šä¸€è¼ªçš„ç„¡æä¸Šä¸‹æ–‡ã€‚")

    prompt_template_runnable = ai_core.get_unified_generation_chain()
    
    final_chain = prompt_template_runnable
    
    final_response_raw = await ai_core.ainvoke_with_rotation(
        final_chain,
        {
            "core_protocol_prompt": ai_core.core_protocol_prompt,
            "response_style_prompt": ai_core.profile.response_style_prompt or "é è¨­é¢¨æ ¼",
            "historical_context": historical_context,
            "world_snapshot": world_snapshot,
            "latest_user_input": user_input,
        },
        retry_strategy='force',
        use_degradation=True
    )

    final_response = final_response_raw.content if hasattr(final_response_raw, 'content') else str(final_response_raw)

    if not final_response or not final_response.strip():
        logger.critical(f"[{user_id}] (Graph|6) æ ¸å¿ƒç”Ÿæˆé“¾åœ¨æŒ‡ä»¤è½°ç‚¸å’Œæ¨¡å‹é™çº§åæœ€ç»ˆå¤±è´¥ï¼")
        final_response = "ï¼ˆæŠ±æ­‰ï¼Œæˆ‘å¥½åƒçªç„¶æ–­çº¿äº†ï¼Œè„‘æµ·ä¸­ä¸€ç‰‡ç©ºç™½... è¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºæ‚¨çš„æŒ‡ä»¤è§¦å‘äº†æ— æ³•ç»•è¿‡çš„æ ¸å¿ƒå†…å®¹å®‰å…¨é™åˆ¶ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªæš‚æ—¶çš„ã€ä¸¥é‡çš„ API æœåŠ¡é—®é¢˜ã€‚è¯·å°è¯•ç”¨å®Œå…¨ä¸åŒçš„æ–¹å¼è¡¨è¾¾æ‚¨çš„æ„å›¾ï¼Œæˆ–ç¨åå†è¯•ã€‚ï¼‰"
        
    logger.info(f"[{user_id}] (Graph|6) æœ€ç»ˆç”Ÿæˆæµç¨‹å®Œæˆã€‚")
    return {"llm_response": final_response}
# å‡½å¼ï¼š[æ–°] æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ (v2.0 - ä¼˜åŒ–ä¿¡æ¯é¡ºåº)




# å‡½å¼ï¼šé©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–ç¯€é»
# æ›´æ–°ç´€éŒ„:
# v7.0 (2025-10-05): [é‡å¤§æ¶æ§‹é‡æ§‹] æ ¹æ®æœ€ç»ˆç¡®ç«‹çš„ v7.0 è“å›¾ï¼Œå½»åº•é‡å†™äº†æ•´ä¸ªå¯¹è¯å›¾ã€‚åºŸå¼ƒäº†æ‰€æœ‰åŸºäº TurnPlan JSON çš„å¤æ‚è§„åˆ’å’Œæ¸²æŸ“èŠ‚ç‚¹ã€‚æ–°çš„â€œä¿¡æ¯æ³¨å…¥å¼æ¶æ„â€æµç¨‹æ›´çº¿æ€§ã€æ›´ç®€å•ï¼š1. æ„ŸçŸ¥ä¸ä¿¡æ¯æ”¶é›†ã€‚ 2. (å…¨æ–°) å‰ç½®å·¥å…·è°ƒç”¨ï¼Œç”¨äºå¤„ç†æ˜ç¡®çš„çŠ¶æ€å˜æ›´ã€‚ 3. å°†æ‰€æœ‰ä¿¡æ¯ï¼ˆLOREã€è®°å¿†ã€å·¥å…·ç»“æœï¼‰ç»„è£…æˆä¸€ä¸ªå·¨å¤§çš„ world_snapshot ä¸Šä¸‹æ–‡ã€‚ 4. (å…¨æ–°) å•ä¸€çš„æœ€ç»ˆç”ŸæˆèŠ‚ç‚¹ï¼Œå°† world_snapshot å’Œç”¨æˆ·æŒ‡ä»¤ç›´æ¥äº¤ç»™ä¸€ä¸ªç”± 00_supreme_directive.txt é©±åŠ¨çš„å¼ºå¤§ LLM è¿›è¡Œä¸€æ­¥åˆ°ä½çš„è‡ªç”±åˆ›ä½œã€‚æ¯ä¸ªä¸ API äº¤äº’çš„èŠ‚ç‚¹éƒ½å†…ç½®äº†å¼ºå¤§çš„â€œåŠŸèƒ½é‡å»ºâ€å¼å¤‡æ´æ–¹æ¡ˆã€‚
# v2.0 (2025-10-07): [æ¶æ§‹é‡æ§‹] æ­¤èŠ‚ç‚¹çš„èŒè´£è¢«æ‰©å±•ã€‚å®ƒç°åœ¨è´Ÿè´£ç»„è£…æ‰€æœ‰ä¸åŒæ¥æºçš„ä¸Šä¸‹æ–‡ï¼ˆRAG è®°å¿†ã€çŸ­æœŸå¯¹è¯å†å²ã€ä¸–ç•Œå¿«ç…§ï¼‰ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§â€œå†å² -> äº‹å® -> æŒ‡ä»¤â€çš„é¡ºåºï¼Œå°†å®ƒä»¬å¡«å……åˆ°æ–°çš„æç¤ºè¯æ¨¡æ¿ä¸­ï¼Œç„¶åè°ƒç”¨æ ¸å¿ƒç”Ÿæˆé“¾ã€‚
# v2.1 (2025-10-14): [ç½é›£æ€§BUGä¿®å¾©] ç¢ºä¿ `llm_response` åœ¨èª¿ç”¨ `.strip()` ä¹‹å‰ï¼Œå…ˆç²å–å…¶ `.content` å±¬æ€§ï¼Œè§£æ±º `AttributeError: 'AIMessage' object has no attribute 'strip'`ã€‚
# v2.2 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] æ¢å¾©äº†å° `ai_core._save_interaction_to_dbs` çš„èª¿ç”¨ï¼Œä»¥ç¢ºä¿å°è©±æ­·å²è¢«æ­£ç¢ºæŒä¹…åŒ–ã€‚
# v2.3 (2025-10-15): [ç½é›£æ€§BUGä¿®å¾©] åœ¨èª¿ç”¨ `lore_extraction_chain` æ™‚ï¼Œè£œå…¨äº†ç¼ºå¤±çš„ `username` å’Œ `ai_name` åƒæ•¸ï¼Œè§£æ±ºäº† `KeyError`ã€‚
# v2.4 (2025-10-15): [å¥å£¯æ€§] åœ¨èª¿ç”¨ `lore_extraction_chain` ä¹‹å‰ï¼Œå…ˆå°å¯èƒ½åŒ…å« NSFW å…§å®¹çš„ `clean_response` é€²è¡Œæ¸…æ´—ï¼Œä»¥é¿å…å…§å®¹å¯©æŸ¥ã€‚
# v2.5 (2025-10-15): [å¥å£¯æ€§] å°‡æœ¬å›åˆçš„æœ€çµ‚å›æ‡‰å­˜å…¥ stateï¼Œç‚ºä¸‹ä¸€è¼ªçš„é€£çºŒæ€§æŒ‡ä»¤æä¾›ç„¡æä¸Šä¸‹æ–‡ã€‚
async def validate_and_persist_node(state: ConversationGraphState) -> Dict:
    """[7] æ¸…ç†æ–‡æœ¬ã€äº‹å¾Œ LORE æå–ã€ä¿å­˜å°è©±æ­·å²ï¼Œä¸¦ç‚ºä¸‹ä¸€è¼ªæº–å‚™ç„¡æä¸Šä¸‹æ–‡ã€‚"""
    user_id = state['user_id']
    ai_core = state['ai_core']
    user_input = state['messages'][-1].content
    llm_response_raw = state['llm_response']
    logger.info(f"[{user_id}] (Graph|7) Node: validate_and_persist -> æ­£åœ¨é©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–...")

    # 1. é©—è­‰èˆ‡æ¸…ç†
    if hasattr(llm_response_raw, 'content'):
        llm_response = llm_response_raw.content
    else:
        llm_response = str(llm_response_raw)
        logger.warning(f"[{user_id}] (Graph|7) LLM å›æ‡‰ä¸æ˜¯ AIMessage ç‰©ä»¶ï¼Œç›´æ¥è½‰æ›ç‚ºå­—ç¬¦ä¸²ã€‚")

    clean_response = llm_response.strip()
    
    # 2. å­¸ç¿’ (äº‹å¾Œ LORE æå–)
    try:
        logger.info(f"[{user_id}] (Graph|7) æ­£åœ¨å•Ÿå‹•äº‹å¾Œ LORE å­¸ç¿’...")
        lore_extraction_chain = ai_core.get_lore_extraction_chain()
        if lore_extraction_chain and ai_core.profile:
            logger.info(f"[{user_id}] (Graph|7) [LORE Pre-Sanitization] æ­£åœ¨ç‚º LORE æå–å™¨æº–å‚™å®‰å…¨çš„è¼¸å…¥æ–‡æœ¬...")
            literary_chain = ai_core.get_literary_euphemization_chain()
            safe_response_for_lore = await ai_core.ainvoke_with_rotation(
                literary_chain,
                {"dialogue_history": clean_response},
                retry_strategy='none'
            )

            if not safe_response_for_lore:
                logger.warning(f"[{user_id}] (Graph|7) [LORE Pre-Sanitization] æ–‡æœ¬é æ¸…æ´—å¤±æ•—ï¼Œå°‡è·³éæœ¬è¼ª LORE æå–ã€‚")
            else:
                logger.info(f"[{user_id}] (Graph|7) [LORE Pre-Sanitization] æ–‡æœ¬é æ¸…æ´—æˆåŠŸã€‚")
                lore_extraction_params = {
                    "username": ai_core.profile.user_profile.name,
                    "ai_name": ai_core.profile.ai_profile.name,
                    "existing_lore_summary": "",
                    "user_input": user_input,
                    "final_response_text": safe_response_for_lore
                }
                extraction_plan = await ai_core.ainvoke_with_rotation(
                    lore_extraction_chain,
                    lore_extraction_params,
                    retry_strategy='euphemize'
                )
                if extraction_plan and extraction_plan.plan:
                    logger.info(f"[{user_id}] (Graph|7) äº‹å¾Œå­¸ç¿’åˆ° {len(extraction_plan.plan)} æ¢æ–° LOREï¼Œæ­£åœ¨å¾Œå°ä¿å­˜...")
                    asyncio.create_task(ai_core._execute_tool_call_plan(extraction_plan, ai_core.profile.game_state.location_path))
    except Exception as e:
        logger.warning(f"[{user_id}] (Graph|7) äº‹å¾Œ LORE å­¸ç¿’å¤±æ•—ï¼Œå·²è·³éã€‚æ ¸å¿ƒå°è©±ä¿å­˜ä¸å—å½±éŸ¿ã€‚éŒ¯èª¤: {e}")

    # 3. æŒä¹…åŒ–
    if clean_response and "æŠ±æ­‰" not in clean_response:
        chat_history_manager = ai_core.session_histories.setdefault(user_id, ChatMessageHistory())
        chat_history_manager.add_user_message(user_input)
        chat_history_manager.add_ai_message(clean_response)
        
        last_interaction_text = f"ä½¿ç”¨è€…: {user_input}\n\nAI:\n{clean_response}"
        asyncio.create_task(ai_core._save_interaction_to_dbs(last_interaction_text))
        
        logger.info(f"[{user_id}] (Graph|7) å°è©±æ­·å²å·²æ›´æ–°ä¸¦æº–å‚™ä¿å­˜åˆ° DBã€‚")

    logger.info(f"[{user_id}] (Graph|7) ç‹€æ…‹æŒä¹…åŒ–å®Œæˆã€‚")
    
    # [v2.5 æ ¸å¿ƒä¿®æ­£] å°‡æœ¬å›åˆçš„æœ€çµ‚å›æ‡‰å­˜å…¥ stateï¼Œç‚ºä¸‹ä¸€è¼ªæä¾›ç„¡æä¸Šä¸‹æ–‡
    return {"final_output": clean_response, "last_response_text": clean_response}
# å‡½å¼ï¼šé©—è­‰ã€å­¸ç¿’èˆ‡æŒä¹…åŒ–ç¯€é»


# å‡½å¼ï¼šç²å–æ‘˜è¦å¾Œçš„å°è©±æ­·å² (v28.0 - çµ‚æ¥µå‚™æ´ä¿®æ­£)

async def _get_summarized_chat_history(ai_core: AILover, user_id: str, num_messages: int = 8) -> str:
    """
    æå–ä¸¦æ‘˜è¦æœ€è¿‘çš„å°è©±æ­·å²ï¼Œä¸¦å…§å»ºä¸€å€‹å¼·å¤§çš„ã€åŸºæ–¼ã€Œæ–‡å­¸è©•è«–å®¶ã€é‡å¯«çš„ NSFW å…§å®¹å®‰å…¨å‚™æ´æ©Ÿåˆ¶ã€‚
    """
    if not ai_core.profile: return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
    chat_history_manager = ai_core.session_histories.get(user_id, ChatMessageHistory())
    if not chat_history_manager.messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"
        
    recent_messages = chat_history_manager.messages[-num_messages:]
    if not recent_messages:
        return "ï¼ˆæ²’æœ‰æœ€è¿‘çš„å°è©±æ­·å²ï¼‰"

    raw_history_text = "\n".join([f"{'ä½¿ç”¨è€…' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" for m in recent_messages])

    try:
        # å°è¯•ç›´æ¥æ‘˜è¦
        literary_chain = ai_core.get_literary_euphemization_chain() # ä½¿ç”¨è¿™ä¸ªæ›´å¼ºå¤§çš„é“¾è¿›è¡Œæ‘˜è¦
        summary = await ai_core.ainvoke_with_rotation(literary_chain, {"dialogue_history": raw_history_text}, retry_strategy='euphemize')

        if not summary or not summary.strip():
            raise Exception("Summarization returned empty content.")
            
        return f"ã€æœ€è¿‘å°è©±æ‘˜è¦ã€‘:\n{summary}"

    except Exception as e:
        logger.error(f"[{user_id}] (History Summarizer) ç”Ÿæˆæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚è¿”å›ä¸­æ€§æç¤ºã€‚")
        return "ï¼ˆæ­·å²å°è©±æ‘˜è¦å› é”™è¯¯è€Œç”Ÿæˆå¤±è´¥ï¼Œéƒ¨åˆ†ä¸Šä¸‹æ–‡å¯èƒ½ç¼ºå¤±ã€‚ï¼‰"
# å‡½å¼ï¼šç²å–æ‘˜è¦å¾Œçš„å°è©±æ­·å² (v28.0 - çµ‚æ¥µå‚™æ´ä¿®æ­£)













# --- [v30.0 æ–°æ¶æ„] å›¾çš„æ„å»º ---
def create_main_response_graph() -> StateGraph:
    """åˆ›å»ºå¹¶è¿æ¥æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ„å»ºæœ€ç»ˆçš„å¯¹è¯å›¾ã€‚"""
    graph = StateGraph(ConversationGraphState)
    
    # æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹
    graph.add_node("perceive_scene", perceive_scene_node)
    graph.add_node("retrieve_and_query", retrieve_and_query_node)
    graph.add_node("expansion_decision_and_execution", expansion_decision_and_execution_node)
    graph.add_node("preemptive_tool_call", preemptive_tool_call_node)
    graph.add_node("assemble_world_snapshot", assemble_world_snapshot_node)
    graph.add_node("final_generation", final_generation_node)
    graph.add_node("validate_and_persist", validate_and_persist_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    graph.set_entry_point("perceive_scene")
    
    # è¿æ¥æµç¨‹
    graph.add_edge("perceive_scene", "retrieve_and_query")
    graph.add_edge("retrieve_and_query", "expansion_decision_and_execution")
    graph.add_edge("expansion_decision_and_execution", "preemptive_tool_call")
    graph.add_edge("preemptive_tool_call", "assemble_world_snapshot")
    graph.add_edge("assemble_world_snapshot", "final_generation")
    graph.add_edge("final_generation", "validate_and_persist")
    graph.add_edge("validate_and_persist", END)
    
    return graph.compile()

# --- æ—§çš„ Setup Graph (ä¿æŒä¸å˜ï¼Œç”¨äº /start æµç¨‹) ---
# ... (æ­¤å¤„åº”åŒ…å« setup_graph çš„æ‰€æœ‰èŠ‚ç‚¹å’Œæ„å»ºå™¨ä»£ç )
# ... (ä¸ºéµå®ˆâ€œä¸¥ç¦çœç•¥â€è§„åˆ™ï¼Œæ­¤å¤„è´´ä¸Šæ‰€æœ‰ setup_graph ç›¸å…³ä»£ç )
async def process_canon_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    canon_text = state['canon_text']
    logger.info(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> ç¯€é»å·²å•Ÿå‹•ã€‚")
    try:
        if canon_text:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æª¢æ¸¬åˆ°ä¸–ç•Œè–ç¶“æ–‡æœ¬ (é•·åº¦: {len(canon_text)})ï¼Œé–‹å§‹è™•ç†...")
            logger.info(f"[{user_id}] (Setup Graph|1/4) æ­¥é©Ÿ A: æ­£åœ¨å‘é‡åŒ–æ–‡æœ¬...")
            await ai_core.add_canon_to_vector_store(canon_text)
            logger.info(f"[{user_id}] (Setup Graph|1/4) æ­¥é©Ÿ A: å‘é‡åŒ–å„²å­˜å®Œæˆã€‚")
            logger.info(f"[{user_id}] (Setup Graph|1/4) æ­¥é©Ÿ B: æ­£åœ¨é€²è¡Œ LORE æ™ºèƒ½è§£æ...")
            await ai_core.parse_and_create_lore_from_canon(None, canon_text, is_setup_flow=True)
            logger.info(f"[{user_id}] (Setup Graph|1/4) æ­¥é©Ÿ B: LORE æ™ºèƒ½è§£æå®Œæˆã€‚")
        else:
            logger.info(f"[{user_id}] (Setup Graph|1/4) æœªæä¾›ä¸–ç•Œè–ç¶“æ–‡æœ¬ï¼Œè·³éè™•ç†ã€‚")
        logger.info(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|1/4) Node: process_canon -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    finally:
        delay_seconds = 5.0
        logger.info(f"[{user_id}] (Setup Graph|1/4|Flow Control) ç‚ºå¹³æ»‘ API è«‹æ±‚ï¼Œå°‡å¼·åˆ¶ç­‰å¾… {delay_seconds} ç§’å¾Œé€²å…¥ä¸‹ä¸€ç¯€é»...")
        await asyncio.sleep(delay_seconds)
    return {}

async def complete_profiles_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»å·²å•Ÿå‹•ï¼Œæº–å‚™è£œå®Œè§’è‰²æª”æ¡ˆ...")
    try:
        if not ai_core.profile:
            logger.error(f"[{user_id}] (Setup Graph|2/4) ai_core.profile ç‚ºç©ºï¼Œç„¡æ³•ç¹¼çºŒã€‚")
            return {}
        completion_chain = ai_core.get_profile_completion_chain()
        literary_chain = ai_core.get_literary_euphemization_chain()
        async def _safe_complete_profile(original_profile: CharacterProfile) -> CharacterProfile:
            try:
                safe_profile_data = original_profile.model_dump()
                description_to_clean = safe_profile_data.get('description', '')
                appearance_to_clean = safe_profile_data.get('appearance', '')
                tasks_to_clean = {}
                if description_to_clean.strip(): tasks_to_clean['description'] = literary_chain.ainvoke({"dialogue_history": description_to_clean})
                if appearance_to_clean.strip(): tasks_to_clean['appearance'] = literary_chain.ainvoke({"dialogue_history": appearance_to_clean})
                if tasks_to_clean:
                    cleaned_results = await asyncio.gather(*tasks_to_clean.values(), return_exceptions=True)
                    results_dict = dict(zip(tasks_to_clean.keys(), cleaned_results))
                    if 'description' in results_dict and isinstance(results_dict['description'], str): safe_profile_data['description'] = results_dict['description']
                    if 'appearance' in results_dict and isinstance(results_dict['appearance'], str): safe_profile_data['appearance'] = results_dict['appearance']
                completed_safe_profile = await ai_core.ainvoke_with_rotation(completion_chain, {"profile_json": json.dumps(safe_profile_data, ensure_ascii=False)}, retry_strategy='euphemize')
                if not completed_safe_profile: return original_profile
                original_data = original_profile.model_dump()
                completed_data = completed_safe_profile.model_dump()
                for key, value in completed_data.items():
                    if not original_data.get(key) or original_data.get(key) in [[], {}, "æœªè¨­å®š", "æœªçŸ¥", ""]:
                        if value: original_data[key] = value
                original_data['description'] = original_profile.description
                original_data['appearance'] = original_profile.appearance
                original_data['name'] = original_profile.name
                return CharacterProfile.model_validate(original_data)
            except Exception as e:
                logger.error(f"[{user_id}] ç‚ºè§’è‰² '{original_profile.name}' é€²è¡Œå®‰å…¨è£œå®Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                return original_profile
        completed_user_profile_task = _safe_complete_profile(ai_core.profile.user_profile)
        completed_ai_profile_task = _safe_complete_profile(ai_core.profile.ai_profile)
        final_user_profile, final_ai_profile = await asyncio.gather(completed_user_profile_task, completed_ai_profile_task)
        await ai_core.update_and_persist_profile({'user_profile': final_user_profile.model_dump(), 'ai_profile': final_ai_profile.model_dump()})
        logger.info(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|2/4) Node: complete_profiles_node -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    finally:
        delay_seconds = 5.0
        logger.info(f"[{user_id}] (Setup Graph|2/4|Flow Control) ç‚ºå¹³æ»‘ API è«‹æ±‚ï¼Œå°‡å¼·åˆ¶ç­‰å¾… {delay_seconds} ç§’å¾Œé€²å…¥ä¸‹ä¸€ç¯€é»...")
        await asyncio.sleep(delay_seconds)
    return {}

async def world_genesis_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»å·²å•Ÿå‹•...")
    genesis_result = None
    try:
        if not ai_core.profile: raise Exception("AI Profile is not loaded.")
        genesis_chain = ai_core.get_world_genesis_chain()
        genesis_result = await ai_core.ainvoke_with_rotation(genesis_chain, {"world_settings": ai_core.profile.world_settings, "username": ai_core.profile.user_profile.name, "ai_name": ai_core.profile.ai_profile.name}, retry_strategy='force')
        if not genesis_result: raise Exception("ä¸–ç•Œå‰µä¸–éˆè¿”å›äº†ç©ºçµæœã€‚")
        gs = ai_core.profile.game_state
        gs.location_path = genesis_result.location_path
        await ai_core.update_and_persist_profile({'game_state': gs.model_dump()})
        await lore_book.add_or_update_lore(user_id, 'location_info', " > ".join(genesis_result.location_path), genesis_result.location_info.model_dump())
        for npc in genesis_result.initial_npcs:
            npc_key = " > ".join(genesis_result.location_path) + f" > {npc.name}"
            await lore_book.add_or_update_lore(user_id, 'npc_profile', npc_key, npc.model_dump())
        logger.info(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|3/4) Node: world_genesis -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
    finally:
        delay_seconds = 5.0
        logger.info(f"[{user_id}] (Setup Graph|3/4|Flow Control) ç‚ºå¹³æ»‘ API è«‹æ±‚ï¼Œå°‡å¼·åˆ¶ç­‰å¾… {delay_seconds} ç§’å¾Œé€²å…¥ä¸‹ä¸€ç¯€é»...")
        await asyncio.sleep(delay_seconds)
    return {"genesis_result": genesis_result}

async def generate_opening_scene_node(state: SetupGraphState) -> Dict:
    user_id = state['user_id']
    ai_core = state['ai_core']
    opening_scene = ""
    logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»å·²å•Ÿå‹•...")
    try:
        opening_scene = await ai_core.generate_opening_scene()
        if not opening_scene or not opening_scene.strip():
            opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
        logger.info(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> ç¯€é»åŸ·è¡ŒæˆåŠŸã€‚")
    except Exception as e:
        logger.error(f"[{user_id}] (Setup Graph|4/4) Node: generate_opening_scene -> åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        opening_scene = (f"åœ¨ä¸€ç‰‡æŸ”å’Œçš„å…‰èŠ’ä¸­...")
    return {"opening_scene": opening_scene}

def create_setup_graph() -> StateGraph:
    """å‰µå»ºè¨­å®šåœ–"""
    graph = StateGraph(SetupGraphState)
    graph.add_node("process_canon", process_canon_node)
    graph.add_node("complete_profiles", complete_profiles_node)
    graph.add_node("world_genesis", world_genesis_node)
    graph.add_node("generate_opening_scene", generate_opening_scene_node)
    graph.set_entry_point("process_canon")
    graph.add_edge("process_canon", "complete_profiles")
    graph.add_edge("complete_profiles", "world_genesis")
    graph.add_edge("world_genesis", "generate_opening_scene")
    graph.add_edge("generate_opening_scene", END)
    return graph.compile()













